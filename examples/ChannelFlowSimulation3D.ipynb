{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import lettuce as lt\n",
    "from lettuce import D3Q19, Lattice, UnitConversion\n",
    "import csv"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-25T06:03:27.380547944Z",
     "start_time": "2025-06-25T06:03:26.172420263Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "\n",
    "class WallQuantitiesInternal:\n",
    "    def __init__(\n",
    "        self,\n",
    "        lattice,\n",
    "        flow,\n",
    "        molecular_viscosity,\n",
    "        y_lattice=1.0,\n",
    "        wall='bottom',\n",
    "        kappa=0.41,\n",
    "        B=5.2,\n",
    "        max_iter=100,\n",
    "        tol=1e-8,\n",
    "        use_smagorinsky=False,\n",
    "        smagorinsky_collision_instance=None\n",
    "    ):\n",
    "        self.lattice = lattice\n",
    "        self.flow = flow\n",
    "        self.nu = molecular_viscosity\n",
    "        self.y = y_lattice\n",
    "        self.wall = wall\n",
    "        self.kappa = kappa\n",
    "        self.B = B\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.normal_axis = 1  # y-Achse\n",
    "\n",
    "        self.use_smagorinsky = use_smagorinsky\n",
    "        if self.use_smagorinsky:\n",
    "            if smagorinsky_collision_instance is None:\n",
    "                raise ValueError(\"Smagorinsky collision instance required if use_smagorinsky=True.\")\n",
    "            self.smagorinsky_collision = smagorinsky_collision_instance\n",
    "\n",
    "    def spalding_law(self, y_plus_grid_dist, u_mag_wall_parallel, nu_effective):\n",
    "        y_plus_grid_dist = torch.tensor(y_plus_grid_dist, device=u_mag_wall_parallel.device, dtype=u_mag_wall_parallel.dtype)\n",
    "        u_plus = (y_plus_grid_dist * u_mag_wall_parallel / nu_effective).clamp(min=1e-4).detach().clone().requires_grad_(False)\n",
    "\n",
    "        for _ in range(self.max_iter):\n",
    "            ku = self.kappa * u_plus\n",
    "            exp_ku = torch.exp(ku)\n",
    "            spalding_term = u_plus + torch.exp(torch.tensor(-self.kappa * self.B, device=u_plus.device, dtype=u_plus.dtype)) * (\n",
    "                exp_ku - 1 - ku - 0.5 * ku**2 - (1/6) * ku**3\n",
    "            )\n",
    "            rhs = (y_plus_grid_dist * u_mag_wall_parallel) / (nu_effective * u_plus.clamp(min=1e-8))\n",
    "            f_eq_solve = spalding_term - rhs\n",
    "\n",
    "            d_spalding_term = 1 + torch.exp(torch.tensor(-self.kappa * self.B, device=u_plus.device, dtype=u_plus.dtype)) * (\n",
    "                self.kappa * exp_ku - self.kappa - self.kappa**2 * u_plus - 0.5 * self.kappa**3 * u_plus**2\n",
    "            )\n",
    "            d_rhs = - (y_plus_grid_dist * u_mag_wall_parallel) / (nu_effective * u_plus.clamp(min=1e-8)**2)\n",
    "            df_eq_solve = d_spalding_term - d_rhs\n",
    "\n",
    "            delta = f_eq_solve / torch.where(torch.abs(df_eq_solve) < 1e-10, torch.tensor(1e-10, device=f_eq_solve.device), df_eq_solve)\n",
    "            u_plus = (u_plus - delta).clamp(min=1e-4)\n",
    "\n",
    "            if torch.max(torch.abs(delta)) < self.tol:\n",
    "                break\n",
    "        return u_plus\n",
    "\n",
    "    def __call__(self, f_full_grid):\n",
    "        rho_full = self.lattice.rho(f_full_grid)\n",
    "        u_full = self.lattice.u(f_full_grid)\n",
    "\n",
    "        if rho_full.ndim == self.lattice.D + 1 and rho_full.shape[0] == 1:\n",
    "            rho_full = rho_full.squeeze(0)\n",
    "        if u_full.ndim == self.lattice.D + 1 and u_full.shape[1] == 1:\n",
    "            u_full = u_full.squeeze(1)\n",
    "\n",
    "        grid_spatial_dims = list(range(self.lattice.D))\n",
    "        spatial_idx_slice = [slice(None)] * self.lattice.D\n",
    "        spatial_idx_slice[self.normal_axis] = 1 if self.wall == \"bottom\" else -2\n",
    "\n",
    "        rho_f = rho_full[tuple(spatial_idx_slice)].flatten()\n",
    "        u_x_f = u_full[0][tuple(spatial_idx_slice)].flatten()\n",
    "        u_y_f = u_full[1][tuple(spatial_idx_slice)].flatten()\n",
    "        u_z_f = u_full[2][tuple(spatial_idx_slice)].flatten() if self.lattice.D == 3 else torch.zeros_like(u_x_f)\n",
    "\n",
    "        u_mag_wall_parallel = torch.sqrt(u_x_f**2 + u_z_f**2).clamp(min=1e-10)\n",
    "\n",
    "        if self.use_smagorinsky:\n",
    "            tau_eff_scalar = self.smagorinsky_collision.tau_eff\n",
    "            shape = u_full[0].shape\n",
    "            tau_eff_full_grid = torch.full(shape, tau_eff_scalar, device=u_full.device, dtype=u_full.dtype)\n",
    "            nu_eff_full_grid = (tau_eff_full_grid - 0.5) / 3.0\n",
    "            nu_eff_wall_layer = nu_eff_full_grid[tuple(spatial_idx_slice)].flatten()\n",
    "        else:\n",
    "            nu_eff_wall_layer = torch.full_like(u_mag_wall_parallel, self.nu)\n",
    "\n",
    "        u_plus = self.spalding_law(self.y, u_mag_wall_parallel, nu_eff_wall_layer)\n",
    "        u_tau = (u_mag_wall_parallel / u_plus).clamp(min=1e-8)\n",
    "        tau_w = rho_f * u_tau**2\n",
    "\n",
    "        safe_u_mag = torch.where(u_mag_wall_parallel < 1e-10, torch.tensor(1.0, device=u_mag_wall_parallel.device, dtype=u_mag_wall_parallel.dtype), u_mag_wall_parallel)\n",
    "        tau_x = - (u_x_f / safe_u_mag) * tau_w\n",
    "        tau_z = - (u_z_f / safe_u_mag) * tau_w\n",
    "\n",
    "        tau_x_wall = torch.zeros_like(u_full[0])\n",
    "        tau_z_wall = torch.zeros_like(u_full[2] if self.lattice.D == 3 else u_full[0])\n",
    "\n",
    "        if self.lattice.D == 3:\n",
    "            target_shape_slice = tau_x_wall[tuple(spatial_idx_slice)].shape\n",
    "            tau_x_wall[tuple(spatial_idx_slice)] = tau_x.reshape(target_shape_slice)\n",
    "            tau_z_wall[tuple(spatial_idx_slice)] = tau_z.reshape(target_shape_slice)\n",
    "        else:\n",
    "            raise ValueError(\"Only 3D supported for this current implementation of WallQuantitiesInternal.\")\n",
    "\n",
    "        return {\n",
    "            \"tau_x\": tau_x_wall,\n",
    "            \"tau_z\": tau_z_wall,\n",
    "            \"u_tau\": u_tau\n",
    "        }\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-25T06:03:27.389768118Z",
     "start_time": "2025-06-25T06:03:27.386472024Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class WallFunctionBoundaryTest:\n",
    "    def __init__(self, mask, lattice, flow, wall='bottom',\n",
    "                 kappa=0.41, B=5.2, max_iter=100, tol=1e-8):\n",
    "        self.mask = lattice.convert_to_tensor(mask)\n",
    "        self.lattice = lattice\n",
    "        self.flow = flow\n",
    "        self.wall = wall\n",
    "        self.kappa = kappa\n",
    "        self.B = B\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "\n",
    "        self.tau_x = None\n",
    "        self.tau_z = None\n",
    "\n",
    "        self.u_tau_mean = torch.tensor(0.0, device=lattice.device, dtype=lattice.dtype)\n",
    "        self.y_plus_mean = torch.tensor(0.0, device=lattice.device, dtype=lattice.dtype)\n",
    "        self.Re_tau_mean = torch.tensor(0.0, device=lattice.device, dtype=lattice.dtype)\n",
    "        self.previous_u_tau_mean = torch.tensor(0.0, device=lattice.device, dtype=lattice.dtype)\n",
    "\n",
    "    def solve_u_tau_exact(self, y, u, nu):\n",
    "        device = u.device\n",
    "        dtype = u.dtype\n",
    "        kappa = self.kappa\n",
    "        B = self.B\n",
    "        A = torch.exp(torch.tensor(-kappa * B, device=device, dtype=dtype))\n",
    "\n",
    "        u_tau = u.clone().clamp(min=1e-4)\n",
    "\n",
    "        for _ in range(self.max_iter):\n",
    "            u_plus = u / u_tau\n",
    "            ku = kappa * u_plus\n",
    "            exp_ku = torch.exp(ku)\n",
    "\n",
    "            f_rhs = u_plus + A * (exp_ku - 1 - ku - 0.5 * ku ** 2 - (1 / 6) * ku ** 3)\n",
    "            lhs = y * u_tau / nu\n",
    "            residual = lhs - f_rhs\n",
    "\n",
    "            d_f_rhs_duplus = 1 + A * (kappa * exp_ku - kappa - kappa**2 * u_plus - 0.5 * kappa**3 * u_plus**2)\n",
    "            d_uplus_du_tau = -u / u_tau**2\n",
    "            df_du_tau = d_f_rhs_duplus * d_uplus_du_tau\n",
    "\n",
    "            d_lhs_du_tau = y / nu\n",
    "            total_derivative = d_lhs_du_tau - df_du_tau\n",
    "\n",
    "            safe_deriv = torch.where(torch.abs(total_derivative) < 1e-12,\n",
    "                                     torch.tensor(1e-12, device=device, dtype=dtype),\n",
    "                                     total_derivative)\n",
    "\n",
    "            delta = residual / safe_deriv\n",
    "            u_tau_new = (u_tau - delta).clamp(min=1e-5)\n",
    "\n",
    "            if torch.max(torch.abs(delta)) < self.tol:\n",
    "                break\n",
    "\n",
    "            u_tau = u_tau_new\n",
    "\n",
    "        return u_tau\n",
    "\n",
    "    def __call__(self, f):\n",
    "        if self.wall == 'bottom':\n",
    "            f17_old = f[17, self.mask].clone()\n",
    "            f16_old = f[16, self.mask].clone()\n",
    "            f10_old = f[10, self.mask].clone()\n",
    "            f8_old  = f[8, self.mask].clone()\n",
    "\n",
    "        elif self.wall == 'top':\n",
    "            f15_old = f[15, self.mask].clone()\n",
    "            f18_old = f[18, self.mask].clone()\n",
    "            f7_old  = f[7, self.mask].clone()\n",
    "            f9_old  = f[9, self.mask].clone()\n",
    "        else:\n",
    "            raise ValueError(\"wall must be 'bottom' or 'top'\")\n",
    "\n",
    "        if self.wall == 'bottom':\n",
    "            mask_fluidcell = torch.zeros_like(self.mask, dtype=torch.bool)\n",
    "            mask_fluidcell[:, 1, :] = self.mask[:, 0, :]\n",
    "        elif self.wall == 'top':\n",
    "            mask_fluidcell = torch.zeros_like(self.mask, dtype=torch.bool)\n",
    "            mask_fluidcell[:, -2, :] = self.mask[:, -1, :]\n",
    "\n",
    "\n",
    "        rho = self.lattice.rho(f)\n",
    "        rho = rho[:,mask_fluidcell]\n",
    "        u = self.lattice.u(f)\n",
    "\n",
    "        u_x = u[0][mask_fluidcell]\n",
    "        u_z = u[2][mask_fluidcell]\n",
    "        u_mag_parallel = torch.sqrt(u_x**2 + u_z**2).clamp(min=1e-10)\n",
    "\n",
    "        y = torch.tensor(1.0, device=f.device, dtype=f.dtype)\n",
    "        nu = torch.tensor(self.flow.units.viscosity_lu, device=f.device, dtype=f.dtype)\n",
    "\n",
    "        if nu <= 0 or torch.isnan(nu) or torch.isinf(nu):\n",
    "            self.previous_u_tau_mean = self.u_tau_mean.clone().detach()\n",
    "            self.u_tau_mean = torch.tensor(0.0, device=f.device, dtype=f.dtype)\n",
    "            self.y_plus_mean = torch.tensor(0.0, device=f.device, dtype=f.dtype)\n",
    "            self.Re_tau_mean = torch.tensor(0.0, device=f.device, dtype=f.dtype)\n",
    "            return f\n",
    "\n",
    "        u_tau = self.solve_u_tau_exact(y, u_mag_parallel, nu)\n",
    "        tau_w = rho * u_tau**2\n",
    "\n",
    "        if torch.isnan(tau_w).any() or torch.isinf(tau_w).any():\n",
    "            self.previous_u_tau_mean = self.u_tau_mean.clone().detach()\n",
    "            self.u_tau_mean = torch.tensor(0.0, device=f.device, dtype=f.dtype)\n",
    "            self.y_plus_mean = torch.tensor(0.0, device=f.device, dtype=f.dtype)\n",
    "            self.Re_tau_mean = torch.tensor(0.0, device=f.device, dtype=f.dtype)\n",
    "            return f\n",
    "\n",
    "        safe_u = torch.where(u_mag_parallel < 1e-10, torch.tensor(1.0, device=f.device), u_mag_parallel)\n",
    "        safe_u = u_mag_parallel\n",
    "        tau_x = - (u_x / safe_u) * tau_w\n",
    "        tau_z = - (u_z / safe_u) * tau_w\n",
    "\n",
    "        tau_x_field = torch.zeros_like(u[0])\n",
    "        tau_z_field = torch.zeros_like(u[2] if self.lattice.D == 3 else u[0])\n",
    "        tau_x_field[self.mask] = 0.5*tau_x\n",
    "        tau_z_field[self.mask] = 0.5*tau_z\n",
    "\n",
    "        f = torch.where(self.mask, f[self.lattice.stencil.opposite], f)\n",
    "\n",
    "        if self.wall == 'bottom':\n",
    "            f[15, self.mask] = f17_old + tau_z_field[self.mask]\n",
    "            f[16, self.mask] = f17_old + tau_z_field[self.mask]\n",
    "            f[18, self.mask] = f16_old - tau_z_field[self.mask]\n",
    "            f[8,  self.mask] = f16_old - tau_z_field[self.mask]\n",
    "            f[7,  self.mask] = f10_old + tau_x_field[self.mask]\n",
    "            f[17, self.mask] = f10_old + tau_x_field[self.mask]\n",
    "            f[9,  self.mask] = f8_old - tau_x_field[self.mask]\n",
    "            f[10, self.mask] = f8_old - tau_x_field[self.mask]\n",
    "        elif self.wall == 'top':\n",
    "            f[17, self.mask] = f15_old + tau_z_field[self.mask]\n",
    "            f[18, self.mask] = f15_old + tau_z_field[self.mask]\n",
    "            f[16, self.mask] = f18_old - tau_z_field[self.mask]\n",
    "            f[9,  self.mask] = f18_old - tau_z_field[self.mask]\n",
    "            f[10, self.mask] = f7_old + tau_x_field[self.mask]\n",
    "            f[15, self.mask] = f7_old + tau_x_field[self.mask]\n",
    "            f[8,  self.mask] = f9_old - tau_x_field[self.mask]\n",
    "            f[7,  self.mask] = f9_old - tau_x_field[self.mask]\n",
    "\n",
    "        self.tau_x = tau_x_field\n",
    "        self.tau_z = tau_z_field\n",
    "        self.previous_u_tau_mean = self.u_tau_mean.clone().detach()\n",
    "        self.u_tau_mean = u_tau.mean()\n",
    "        # Lokales y_plus wie gehabt\n",
    "        self.y_plus_mean = (y * u_tau / nu).mean()\n",
    "\n",
    "        # Korrektes Re_tau\n",
    "        H = self.flow.resolution_y / 2  # für y-Achse\n",
    "        self.Re_tau_mean = (u_tau * H / nu).mean()\n",
    "\n",
    "        print(\"Re tau:\" + str(self.Re_tau_mean))\n",
    "        if torch.isnan(self.u_tau_mean) or torch.isinf(self.u_tau_mean) or \\\n",
    "           torch.isnan(self.y_plus_mean) or torch.isinf(self.y_plus_mean) or \\\n",
    "           torch.isnan(self.Re_tau_mean) or torch.isinf(self.Re_tau_mean):\n",
    "            self.u_tau_mean = torch.tensor(0.0, device=f.device, dtype=f.dtype)\n",
    "            self.y_plus_mean = torch.tensor(0.0, device=f.device, dtype=f.dtype)\n",
    "            self.Re_tau_mean = torch.tensor(0.0, device=f.device, dtype=f.dtype)\n",
    "\n",
    "\n",
    "        return f\n",
    "\n",
    "    def make_no_collision_mask(self, f_shape):\n",
    "        assert self.mask.shape == f_shape[1:]\n",
    "        return self.mask\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-25T06:03:27.466105508Z",
     "start_time": "2025-06-25T06:03:27.392768830Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class WallFunctionBoundaryTest2:\n",
    "    def __init__(self, mask, lattice, flow, wall='bottom', apply_wfb_correction=True,\n",
    "                 smagorinsky_collision_instance=None):\n",
    "        self.mask = lattice.convert_to_tensor(mask)\n",
    "        self.lattice = lattice\n",
    "        self.flow = flow\n",
    "        self.wall = wall\n",
    "        self.apply_wfb_correction = apply_wfb_correction\n",
    "        self.normal_axis = 1\n",
    "\n",
    "        self.tau_x = None\n",
    "        self.tau_z = None\n",
    "        self.u_tau_mean = None\n",
    "        self.y_plus_mean = None\n",
    "        self.Re_tau_mean = None\n",
    "\n",
    "        self.use_smagorinsky = smagorinsky_collision_instance is not None\n",
    "\n",
    "        # Konstruktion der WallQuantitiesInternal – immer\n",
    "        self.wall_quantities_internal = WallQuantitiesInternal(\n",
    "            lattice=self.lattice,\n",
    "            flow=self.flow,\n",
    "            molecular_viscosity=self.flow.units.viscosity_lu,\n",
    "            wall=self.wall,\n",
    "            y_lattice=1.0,\n",
    "            use_smagorinsky=self.use_smagorinsky,\n",
    "            smagorinsky_collision_instance=smagorinsky_collision_instance\n",
    "        )\n",
    "\n",
    "    def set_smagorinsky_collision(self, collision):\n",
    "        self.smagorinsky_collision = collision\n",
    "        self.wall_quantities_internal = WallQuantitiesInternal(\n",
    "            lattice=self.lattice,\n",
    "            flow=self.flow,\n",
    "            molecular_viscosity=self.flow.units.viscosity_lu,\n",
    "            wall=self.wall,\n",
    "            y_lattice=1.0,\n",
    "            smagorinsky_collision_instance=collision\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "    def __call__(self, f):\n",
    "        # --- 1. Klonen der Originalverteilungen für spätere Korrektur ---\n",
    "\n",
    "        if self.wall == 'bottom':\n",
    "            f17_old = f [17, self.mask].clone()\n",
    "            f16_old = f[16, self.mask].clone()\n",
    "            f10_old = f[10, self.mask].clone()\n",
    "            f8_old  = f[8, self.mask].clone()\n",
    "        elif self.wall == 'top':\n",
    "            f15_old = f[15, self.mask].clone()\n",
    "            f18_old = f[18, self.mask].clone()\n",
    "            f7_old  = f[7, self.mask].clone()\n",
    "            f9_old  = f[9, self.mask].clone()\n",
    "        else:\n",
    "            raise ValueError(\"wall must be 'bottom' or 'top'\")\n",
    "\n",
    "        # --- 2. Wall Quantities intern berechnen ---\n",
    "        results = self.wall_quantities_internal(f)\n",
    "        tau_x_field = results[\"tau_x\"]\n",
    "        tau_z_field = results[\"tau_z\"]\n",
    "\n",
    "        # --- 3. Free-Slip Bounce-Back anwenden ---\n",
    "        f = torch.where(self.mask, f[self.lattice.stencil.opposite], f)\n",
    "\n",
    "        # --- 4. Additive Wandkorrektur ---\n",
    "        if self.wall == 'bottom':\n",
    "            f[15, self.mask] = f17_old + tau_z_field[self.mask]\n",
    "            f[16, self.mask] = f17_old + tau_z_field[self.mask]\n",
    "            f[18, self.mask] = f16_old - tau_z_field[self.mask]\n",
    "            f[8,  self.mask] = f16_old - tau_z_field[self.mask]\n",
    "            f[7,  self.mask] = f10_old + tau_x_field[self.mask]\n",
    "            f[17, self.mask] = f10_old + tau_x_field[self.mask]\n",
    "            f[9,  self.mask] = f8_old - tau_x_field[self.mask]\n",
    "            f[10, self.mask] = f8_old - tau_x_field[self.mask]\n",
    "        elif self.wall == 'top':\n",
    "            f[17, self.mask] = f15_old + tau_z_field[self.mask]\n",
    "            f[18, self.mask] = f15_old + tau_z_field[self.mask]\n",
    "            f[16, self.mask] = f18_old - tau_z_field[self.mask]\n",
    "            f[9,  self.mask] = f18_old - tau_z_field[self.mask]\n",
    "            f[10, self.mask] = f7_old + tau_x_field[self.mask]\n",
    "            f[15, self.mask] = f7_old + tau_x_field[self.mask]\n",
    "            f[8,  self.mask] = f9_old - tau_x_field[self.mask]\n",
    "            f[7,  self.mask] = f9_old - tau_x_field[self.mask]\n",
    "\n",
    "        # --- 5. Ergebnisse für Reporter speichern ---\n",
    "        self.tau_x = tau_x_field\n",
    "        self.tau_z = tau_z_field\n",
    "\n",
    "\n",
    "        self.u_tau_mean = results[\"u_tau\"].mean()\n",
    "        self.previous_u_tau_mean = self.u_tau_mean.clone().detach()\n",
    "\n",
    "        self.y_plus_mean = (self.wall_quantities_internal.y * results[\"u_tau\"] / self.wall_quantities_internal.nu).mean()\n",
    "        self.Re_tau_mean = (results[\"u_tau\"] * self.wall_quantities_internal.y / self.wall_quantities_internal.nu).mean()\n",
    "\n",
    "        return f\n",
    "\n",
    "    def make_no_collision_mask(self, f_shape):\n",
    "        \"\"\"\n",
    "        Diese Boundary-Methode liefert die Maske der Wandknoten,\n",
    "        auf denen der Kollisionsschritt der Hauptsimulation übersprungen werden soll.\n",
    "        Diese Klasse operiert auf diesen Wandknoten selbst.\n",
    "        \"\"\"\n",
    "        assert self.mask.shape == f_shape[1:]\n",
    "        # KORREKTUR: Muss die Maske der eigenen Wandknoten (self.mask) zurückgeben.\n",
    "        return self.mask\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-25T06:03:27.474094938Z",
     "start_time": "2025-06-25T06:03:27.472952797Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "class ChannelFlow3DTest:\n",
    "    def __init__(self, resolution_x, resolution_y, resolution_z,\n",
    "                 reynolds_number, mach_number, lattice, char_length_lu,\n",
    "                 boundary=None,  # jetzt erlaubt: externe Übergabe\n",
    "                 boundaries=None  # neu: direkt übergebene Instanzen\n",
    "                 ):\n",
    "        self.resolution_x = resolution_x\n",
    "        self.resolution_y = resolution_y\n",
    "        self.resolution_z = resolution_z\n",
    "        self._boundary = boundary  # z. B. \"wallfunction\"\n",
    "        self._external_boundaries = boundaries  # die Instanzen direkt\n",
    "        self._boundaries = None  # wird ggf. erzeugt\n",
    "\n",
    "        self.units = UnitConversion(\n",
    "            lattice,\n",
    "            reynolds_number=reynolds_number,\n",
    "            mach_number=mach_number,\n",
    "            characteristic_length_lu=char_length_lu,\n",
    "            characteristic_length_pu=1,\n",
    "            characteristic_velocity_pu=1\n",
    "        )\n",
    "\n",
    "        self._mask = np.zeros(shape=(self.resolution_x, self.resolution_y, self.resolution_z), dtype=bool)\n",
    "    @property\n",
    "    def mask(self):\n",
    "        return self._mask\n",
    "\n",
    "    @mask.setter\n",
    "    def mask(self, m):\n",
    "        assert isinstance(m, np.ndarray) and m.shape == (self.resolution_x, self.resolution_y, self.resolution_z)\n",
    "        self._mask = m.astype(bool)\n",
    "\n",
    "    def initial_solution(self, grid):\n",
    "        xg, yg, zg = grid\n",
    "        p = np.ones_like(xg)[None, ...]\n",
    "        nx, ny, nz = self.resolution_x, self.resolution_y, self.resolution_z\n",
    "\n",
    "        u = np.zeros((3, nx, ny, nz))\n",
    "\n",
    "        # --- 📐 Poiseuille-Profil (in x-Richtung) ---\n",
    "        y_normalized = yg / yg.max()\n",
    "        u_base = y_normalized * (1 - y_normalized)\n",
    "        u[0] = u_base * (1 - self.mask.astype(float))  # u_x = Basisströmung\n",
    "\n",
    "        # --- 🎛️ Sinusmoden-Störung (3D) ---\n",
    "        A_sin = 0.5  # 5% Amplitude\n",
    "        Lx, Ly, Lz = xg.max(), yg.max(), zg.max()\n",
    "        sinus_modes = [(1, 1, 1), (2, 2, 3), (3, 2, 1)]\n",
    "\n",
    "        for kx, ky, kz in sinus_modes:\n",
    "            phase = 2 * np.pi * np.random.rand()\n",
    "            mode = np.sin(2 * np.pi * (kx * xg / Lx + ky * yg / Ly + kz * zg / Lz) + phase)\n",
    "            envelope = y_normalized * (1 - y_normalized)\n",
    "            u[0] += A_sin * mode * envelope  # nur u_x gestört, kannst du erweitern\n",
    "\n",
    "        # --- 🌪️ Vektorpotential ψ (3 Komponenten für Curl in 3D) ---\n",
    "        A_psi = 1\n",
    "        random_psi = ((np.random.rand(3, nx, ny, nz) - 0.5) * 2)\n",
    "\n",
    "        # Wandgewichtung in y und z\n",
    "        y_weight = np.exp(-((y_normalized - 0.0) / 0.2) ** 2) + np.exp(-((y_normalized - 1.0) / 0.2) ** 2)\n",
    "        y_weight /= y_weight.max()\n",
    "\n",
    "        z_normalized = zg / zg.max()\n",
    "        z_weight = np.exp(-((z_normalized - 0.5) / 0.3) ** 2)\n",
    "        z_weight /= z_weight.max()\n",
    "\n",
    "        weight = y_weight * z_weight\n",
    "        random_psi *= weight[None, :, :, :]\n",
    "\n",
    "        # FFT-Filterung (3D)\n",
    "        k0 = np.sqrt(nx ** 2 + ny ** 2 + nz ** 2)\n",
    "        psi_filtered = np.empty_like(random_psi)\n",
    "        for d in range(3):\n",
    "            psi_hat = np.fft.fftn(random_psi[d])\n",
    "            kx = np.fft.fftfreq(nx).reshape(-1, 1, 1)\n",
    "            ky = np.fft.fftfreq(ny).reshape(1, -1, 1)\n",
    "            kz = np.fft.fftfreq(nz).reshape(1, 1, -1)\n",
    "            kabs = np.sqrt((kx * nx) ** 2 + (ky * ny) ** 2 + (kz * nz) ** 2)\n",
    "            filter_mask = np.exp(-kabs / (0.3 * k0))\n",
    "            psi_hat *= filter_mask\n",
    "            psi_hat[0, 0, 0] = 0\n",
    "            psi_filtered[d] = np.real(np.fft.ifftn(psi_hat))\n",
    "\n",
    "        # --- 🌀 Curl(ψ): u = ∇ × ψ ---\n",
    "        u_psi = np.zeros_like(u)\n",
    "        u_psi[0] = np.gradient(psi_filtered[2], axis=1) - np.gradient(psi_filtered[1], axis=2)  # u_x\n",
    "        u_psi[1] = np.gradient(psi_filtered[0], axis=2) - np.gradient(psi_filtered[2], axis=0)  # u_y\n",
    "        u_psi[2] = np.gradient(psi_filtered[1], axis=0) - np.gradient(psi_filtered[0], axis=1)  # u_z\n",
    "\n",
    "        # Normierung\n",
    "        umax_psi = np.max(np.sqrt(np.sum(u_psi ** 2, axis=0)))\n",
    "        if umax_psi > 0:\n",
    "            u_psi *= A_psi / umax_psi\n",
    "\n",
    "        # --- Überlagerung: Basis + Sine + Curl ---\n",
    "        u += u_psi\n",
    "        # 2D: Nullsetzen der Wandgeschwindigkeit\n",
    "\n",
    "        u[:, :, 0, :] = 0.0  # untere Wand (y=0)\n",
    "        u[:, :, -1, :] = 0.0  # obere Wand (y=Ny-1)\n",
    "\n",
    "        return p, u\n",
    "\n",
    "    @property\n",
    "    def grid(self):\n",
    "        stop_x = self.resolution_x / self.units.characteristic_length_lu\n",
    "        stop_y = self.resolution_y / self.units.characteristic_length_lu\n",
    "        stop_z = self.resolution_z / self.units.characteristic_length_lu\n",
    "\n",
    "        x = np.linspace(0, stop_x, num=self.resolution_x, endpoint=False)\n",
    "        y = np.linspace(0, stop_y, num=self.resolution_y, endpoint=False)\n",
    "        z = np.linspace(0, stop_z, num=self.resolution_z, endpoint=False)\n",
    "\n",
    "        return np.meshgrid(x, y, z, indexing='ij')\n",
    "\n",
    "    @property\n",
    "    def boundaries(self):\n",
    "\n",
    "        return self._external_boundaries"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-25T06:03:27.484915845Z",
     "start_time": "2025-06-25T06:03:27.483648892Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class WallQuantitiesTest:\n",
    "    def __init__(self, lattice, flow, boundary):\n",
    "        self.lattice = lattice\n",
    "        self.flow = flow\n",
    "        self.boundary = boundary\n",
    "\n",
    "    def __call__(self, f):\n",
    "        try:\n",
    "            # Hier die hasattr-Prüfung ENTFERNEN. Die Attribute existieren jetzt immer, da sie in __init__\n",
    "            # von WallFunctionBoundaryTest mit 0.0 initialisiert werden.\n",
    "\n",
    "            # Stattdessen prüfen, ob die Werte noch die initialen Nullen sind oder NaN/Inf\n",
    "            # (assuming WallFunctionBoundaryTest initializes to torch.tensor(0.0))\n",
    "            if (self.boundary.u_tau_mean.item() == 0.0 and self.boundary.y_plus_mean.item() == 0.0 and self.boundary.Re_tau_mean.item() == 0.0) or \\\n",
    "               torch.isnan(self.boundary.u_tau_mean) or torch.isinf(self.boundary.u_tau_mean):\n",
    "                return torch.zeros(3, dtype=f.dtype, device=f.device)\n",
    "\n",
    "            u_tau_mean = self.boundary.u_tau_mean\n",
    "            y_plus_mean = self.boundary.y_plus_mean\n",
    "            re_tau_mean = self.boundary.Re_tau_mean\n",
    "\n",
    "\n",
    "            return torch.stack([\n",
    "                u_tau_mean,\n",
    "                y_plus_mean,\n",
    "                re_tau_mean,\n",
    "            ])\n",
    "\n",
    "        except Exception as e:\n",
    "            return torch.zeros(3, dtype=f.dtype, device=f.device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-25T06:03:27.570725815Z",
     "start_time": "2025-06-25T06:03:27.486699065Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ICH FUNKTIONIERE MIT PULLEN\n"
     ]
    }
   ],
   "source": [
    "from argparse import ArgumentParser, ArgumentDefaultsHelpFormatter\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "parser = ArgumentParser(formatter_class=ArgumentDefaultsHelpFormatter)\n",
    "parser.add_argument(\"--vtkdir\", type=str, default=\"./output/\")\n",
    "parser.add_argument(\"--csvdir\", type=str, default=\"./output/\")\n",
    "parser.add_argument(\"--nout\", type=int, default=100)\n",
    "parser.add_argument(\"--nvtk\", type=int, default=100)\n",
    "parser.add_argument(\"--tmax\", type=int, default=20)\n",
    "parser.add_argument(\"--Re\", type=int, default=13800)\n",
    "parser.add_argument(\"--collision_operator\", type=str, default=\"BGK\")\n",
    "parser.add_argument(\"--Precision\", type=str, default=\"Double\")\n",
    "parser.add_argument(\"--Mach\", type=float, default=0.1)\n",
    "parser.add_argument(\"--h\", type=int, default=20, help=\"Halbe Kanalhöhe in LU\")\n",
    "parser.add_argument(\"--bbtype\", type=str, default=\"wallfunction\", choices=[\"halfway\", \"fullway\", \"wallfunction\", \"freeslip\"],\n",
    "                    help=\"Typ der Bounce-Back-Randbedingung\")\n",
    "\n",
    "args, unknown = parser.parse_known_args()\n",
    "args = vars(args)\n",
    "\n",
    "print(\"ICH FUNKTIONIERE MIT PULLEN\")\n",
    "\n",
    "\n",
    "\n",
    "# Einheiten und Auflösung\n",
    "h = args[\"h\"]                      # Kanalhalbhöhe in LU\n",
    "res_y = 2 * h                     # y: volle Kanalhöhe\n",
    "res_x = int(2*np.pi * h)\n",
    "res_z = int(np.pi * h)\n",
    "\n",
    "# Restliche Parameter\n",
    "Re = args[\"Re\"]\n",
    "basedir = args[\"vtkdir\"]\n",
    "csvdir = args[\"csvdir\"]\n",
    "nout = args[\"nout\"]\n",
    "nvtk = args[\"nvtk\"]\n",
    "tmax = args[\"tmax\"]\n",
    "Precision = args[\"Precision\"]\n",
    "collision_operator = args[\"collision_operator\"]\n",
    "Mach = args[\"Mach\"]\n",
    "bbtype = args[\"bbtype\"]\n",
    "# Präzision\n",
    "if Precision == \"Single\":\n",
    "    dtype = torch.float32\n",
    "elif Precision == \"Double\":\n",
    "    dtype = torch.float64\n",
    "elif Precision == \"Half\":\n",
    "    dtype = torch.float16\n",
    "\n",
    "\n",
    "Re_tau = 180\n",
    "\n",
    "smagorinsky_constant = 0.17\n",
    "\n",
    "delta_x = 1.0"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-25T06:03:27.570965921Z",
     "start_time": "2025-06-25T06:03:27.528473249Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA ist nicht verfügbar. Verwende CPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ben/anaconda3/envs/lettuce/lib/python3.11/site-packages/torch/cuda/__init__.py:138: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at /opt/conda/conda-bld/pytorch_1695392035891/work/c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps     time     GlobalMeanUXReporter\n",
      "steps     time     WallQuantities\n",
      "steps     time     WallQuantities\n",
      "steps     time     IncompressibleKineticEnergy\n",
      "Re tau:tensor(100.5786, dtype=torch.float64)\n",
      "Re tau:tensor(100.7155, dtype=torch.float64)\n",
      "Re tau:tensor(97.6644, dtype=torch.float64)\n",
      "Re tau:tensor(97.4102, dtype=torch.float64)\n",
      "Re tau:tensor(100.9864, dtype=torch.float64)\n",
      "Re tau:tensor(100.1172, dtype=torch.float64)\n",
      "Re tau:tensor(104.9008, dtype=torch.float64)\n",
      "Re tau:tensor(103.7616, dtype=torch.float64)\n",
      "Re tau:tensor(104.4983, dtype=torch.float64)\n",
      "Re tau:tensor(103.8782, dtype=torch.float64)\n",
      "Re tau:tensor(107.1384, dtype=torch.float64)\n",
      "Re tau:tensor(107.0454, dtype=torch.float64)\n",
      "Re tau:tensor(104.5942, dtype=torch.float64)\n",
      "Re tau:tensor(103.5846, dtype=torch.float64)\n",
      "Re tau:tensor(107.5952, dtype=torch.float64)\n",
      "Re tau:tensor(106.6295, dtype=torch.float64)\n",
      "Re tau:tensor(106.2460, dtype=torch.float64)\n",
      "Re tau:tensor(104.8944, dtype=torch.float64)\n",
      "Re tau:tensor(109.0273, dtype=torch.float64)\n",
      "Re tau:tensor(107.9215, dtype=torch.float64)\n",
      "Re tau:tensor(108.2454, dtype=torch.float64)\n",
      "Re tau:tensor(106.9158, dtype=torch.float64)\n",
      "Re tau:tensor(112.3386, dtype=torch.float64)\n",
      "Re tau:tensor(111.3345, dtype=torch.float64)\n",
      "Re tau:tensor(110.5019, dtype=torch.float64)\n",
      "Re tau:tensor(108.8393, dtype=torch.float64)\n",
      "Re tau:tensor(112.8254, dtype=torch.float64)\n",
      "Re tau:tensor(111.8083, dtype=torch.float64)\n",
      "Re tau:tensor(111.8108, dtype=torch.float64)\n",
      "Re tau:tensor(110.5137, dtype=torch.float64)\n",
      "Re tau:tensor(116.0582, dtype=torch.float64)\n",
      "Re tau:tensor(114.9869, dtype=torch.float64)\n",
      "Re tau:tensor(114.4623, dtype=torch.float64)\n",
      "Re tau:tensor(113.3798, dtype=torch.float64)\n",
      "Re tau:tensor(118.3785, dtype=torch.float64)\n",
      "Re tau:tensor(116.8837, dtype=torch.float64)\n",
      "Re tau:tensor(115.4824, dtype=torch.float64)\n",
      "Re tau:tensor(114.0301, dtype=torch.float64)\n",
      "Re tau:tensor(119.0402, dtype=torch.float64)\n",
      "Re tau:tensor(117.7211, dtype=torch.float64)\n",
      "Re tau:tensor(118.3765, dtype=torch.float64)\n",
      "Re tau:tensor(116.5988, dtype=torch.float64)\n",
      "Re tau:tensor(121.2416, dtype=torch.float64)\n",
      "Re tau:tensor(119.7023, dtype=torch.float64)\n",
      "Re tau:tensor(120.0886, dtype=torch.float64)\n",
      "Re tau:tensor(118.3024, dtype=torch.float64)\n",
      "Re tau:tensor(123.7194, dtype=torch.float64)\n",
      "Re tau:tensor(121.8993, dtype=torch.float64)\n",
      "Re tau:tensor(121.6145, dtype=torch.float64)\n",
      "Re tau:tensor(119.9899, dtype=torch.float64)\n",
      "Re tau:tensor(124.6511, dtype=torch.float64)\n",
      "Re tau:tensor(122.8337, dtype=torch.float64)\n",
      "Re tau:tensor(123.6320, dtype=torch.float64)\n",
      "Re tau:tensor(121.6889, dtype=torch.float64)\n",
      "Re tau:tensor(126.7120, dtype=torch.float64)\n",
      "Re tau:tensor(124.8138, dtype=torch.float64)\n",
      "Re tau:tensor(125.7178, dtype=torch.float64)\n",
      "Re tau:tensor(123.6238, dtype=torch.float64)\n",
      "Re tau:tensor(128.6024, dtype=torch.float64)\n",
      "Re tau:tensor(126.5367, dtype=torch.float64)\n",
      "Re tau:tensor(127.2483, dtype=torch.float64)\n",
      "Re tau:tensor(125.2259, dtype=torch.float64)\n",
      "Re tau:tensor(130.1865, dtype=torch.float64)\n",
      "Re tau:tensor(128.1274, dtype=torch.float64)\n",
      "Re tau:tensor(129.1539, dtype=torch.float64)\n",
      "Re tau:tensor(126.9379, dtype=torch.float64)\n",
      "Re tau:tensor(132.2111, dtype=torch.float64)\n",
      "Re tau:tensor(129.9493, dtype=torch.float64)\n",
      "Re tau:tensor(130.8335, dtype=torch.float64)\n",
      "Re tau:tensor(128.9778, dtype=torch.float64)\n",
      "Re tau:tensor(133.7803, dtype=torch.float64)\n",
      "Re tau:tensor(131.7859, dtype=torch.float64)\n",
      "Re tau:tensor(132.7753, dtype=torch.float64)\n",
      "Re tau:tensor(130.9322, dtype=torch.float64)\n",
      "Re tau:tensor(135.5347, dtype=torch.float64)\n",
      "Re tau:tensor(133.5778, dtype=torch.float64)\n",
      "Re tau:tensor(134.6886, dtype=torch.float64)\n",
      "Re tau:tensor(133.0347, dtype=torch.float64)\n",
      "Re tau:tensor(137.4354, dtype=torch.float64)\n",
      "Re tau:tensor(135.6734, dtype=torch.float64)\n",
      "Re tau:tensor(136.5358, dtype=torch.float64)\n",
      "Re tau:tensor(134.7776, dtype=torch.float64)\n",
      "Re tau:tensor(139.4190, dtype=torch.float64)\n",
      "Re tau:tensor(137.5238, dtype=torch.float64)\n",
      "Re tau:tensor(138.4179, dtype=torch.float64)\n",
      "Re tau:tensor(136.6470, dtype=torch.float64)\n",
      "Re tau:tensor(141.1672, dtype=torch.float64)\n",
      "Re tau:tensor(139.1610, dtype=torch.float64)\n",
      "Re tau:tensor(140.3804, dtype=torch.float64)\n",
      "Re tau:tensor(138.4812, dtype=torch.float64)\n",
      "Re tau:tensor(142.6624, dtype=torch.float64)\n",
      "Re tau:tensor(140.8053, dtype=torch.float64)\n",
      "Re tau:tensor(141.7463, dtype=torch.float64)\n",
      "Re tau:tensor(139.9811, dtype=torch.float64)\n",
      "Re tau:tensor(144.5004, dtype=torch.float64)\n",
      "Re tau:tensor(142.4007, dtype=torch.float64)\n",
      "Re tau:tensor(143.5477, dtype=torch.float64)\n",
      "Re tau:tensor(141.7138, dtype=torch.float64)\n",
      "Re tau:tensor(146.0722, dtype=torch.float64)\n",
      "Re tau:tensor(143.7606, dtype=torch.float64)\n",
      "Re tau:tensor(145.3066, dtype=torch.float64)\n",
      "Re tau:tensor(143.1508, dtype=torch.float64)\n",
      "Re tau:tensor(147.4284, dtype=torch.float64)\n",
      "Re tau:tensor(145.0544, dtype=torch.float64)\n",
      "Re tau:tensor(146.6707, dtype=torch.float64)\n",
      "Re tau:tensor(144.2004, dtype=torch.float64)\n",
      "Re tau:tensor(149.1453, dtype=torch.float64)\n",
      "Re tau:tensor(146.6575, dtype=torch.float64)\n",
      "Re tau:tensor(148.3899, dtype=torch.float64)\n",
      "Re tau:tensor(146.3785, dtype=torch.float64)\n",
      "Re tau:tensor(150.5650, dtype=torch.float64)\n",
      "Re tau:tensor(147.9176, dtype=torch.float64)\n",
      "Re tau:tensor(150.1725, dtype=torch.float64)\n",
      "Re tau:tensor(147.7264, dtype=torch.float64)\n",
      "Re tau:tensor(152.5109, dtype=torch.float64)\n",
      "Re tau:tensor(149.9092, dtype=torch.float64)\n",
      "Re tau:tensor(151.9059, dtype=torch.float64)\n",
      "Re tau:tensor(149.3041, dtype=torch.float64)\n",
      "Re tau:tensor(154.1458, dtype=torch.float64)\n",
      "Re tau:tensor(151.1383, dtype=torch.float64)\n",
      "Re tau:tensor(153.8095, dtype=torch.float64)\n",
      "Re tau:tensor(151.1873, dtype=torch.float64)\n",
      "Re tau:tensor(155.8366, dtype=torch.float64)\n",
      "Re tau:tensor(153.3603, dtype=torch.float64)\n",
      "Re tau:tensor(155.2493, dtype=torch.float64)\n",
      "Re tau:tensor(152.7942, dtype=torch.float64)\n",
      "Re tau:tensor(157.3712, dtype=torch.float64)\n",
      "Re tau:tensor(154.5168, dtype=torch.float64)\n",
      "Re tau:tensor(156.9400, dtype=torch.float64)\n",
      "Re tau:tensor(154.6658, dtype=torch.float64)\n",
      "Re tau:tensor(159.1370, dtype=torch.float64)\n",
      "Re tau:tensor(156.5444, dtype=torch.float64)\n",
      "Re tau:tensor(158.6096, dtype=torch.float64)\n",
      "Re tau:tensor(156.0545, dtype=torch.float64)\n",
      "Re tau:tensor(160.6999, dtype=torch.float64)\n",
      "Re tau:tensor(157.8807, dtype=torch.float64)\n",
      "Re tau:tensor(160.3048, dtype=torch.float64)\n",
      "Re tau:tensor(157.9199, dtype=torch.float64)\n",
      "Re tau:tensor(162.2656, dtype=torch.float64)\n",
      "Re tau:tensor(159.5562, dtype=torch.float64)\n",
      "Re tau:tensor(161.9130, dtype=torch.float64)\n",
      "Re tau:tensor(159.3805, dtype=torch.float64)\n",
      "Re tau:tensor(163.8884, dtype=torch.float64)\n",
      "Re tau:tensor(161.1519, dtype=torch.float64)\n",
      "Re tau:tensor(163.5567, dtype=torch.float64)\n",
      "Re tau:tensor(161.3736, dtype=torch.float64)\n",
      "Re tau:tensor(165.5210, dtype=torch.float64)\n",
      "Re tau:tensor(162.9977, dtype=torch.float64)\n",
      "Re tau:tensor(165.3251, dtype=torch.float64)\n",
      "Re tau:tensor(162.8632, dtype=torch.float64)\n",
      "Re tau:tensor(167.3077, dtype=torch.float64)\n",
      "Re tau:tensor(164.4466, dtype=torch.float64)\n",
      "Re tau:tensor(167.1371, dtype=torch.float64)\n",
      "Re tau:tensor(164.6580, dtype=torch.float64)\n",
      "Re tau:tensor(169.1284, dtype=torch.float64)\n",
      "Re tau:tensor(166.4087, dtype=torch.float64)\n",
      "Re tau:tensor(168.9615, dtype=torch.float64)\n",
      "Re tau:tensor(166.3926, dtype=torch.float64)\n",
      "Re tau:tensor(170.8081, dtype=torch.float64)\n",
      "Re tau:tensor(167.9420, dtype=torch.float64)\n",
      "Re tau:tensor(170.6878, dtype=torch.float64)\n",
      "Re tau:tensor(168.1449, dtype=torch.float64)\n",
      "Re tau:tensor(172.4304, dtype=torch.float64)\n",
      "Re tau:tensor(169.6620, dtype=torch.float64)\n",
      "Re tau:tensor(172.4283, dtype=torch.float64)\n",
      "Re tau:tensor(169.8820, dtype=torch.float64)\n",
      "Re tau:tensor(174.3310, dtype=torch.float64)\n",
      "Re tau:tensor(171.5342, dtype=torch.float64)\n",
      "Re tau:tensor(174.3703, dtype=torch.float64)\n",
      "Re tau:tensor(171.6902, dtype=torch.float64)\n",
      "Re tau:tensor(176.0613, dtype=torch.float64)\n",
      "Re tau:tensor(173.2052, dtype=torch.float64)\n",
      "Re tau:tensor(176.0163, dtype=torch.float64)\n",
      "Re tau:tensor(173.4103, dtype=torch.float64)\n",
      "Re tau:tensor(177.7216, dtype=torch.float64)\n",
      "Re tau:tensor(174.7213, dtype=torch.float64)\n",
      "Re tau:tensor(177.8660, dtype=torch.float64)\n",
      "Re tau:tensor(175.1233, dtype=torch.float64)\n",
      "Re tau:tensor(179.5074, dtype=torch.float64)\n",
      "Re tau:tensor(176.6117, dtype=torch.float64)\n",
      "Re tau:tensor(179.3762, dtype=torch.float64)\n",
      "Re tau:tensor(176.7787, dtype=torch.float64)\n",
      "Re tau:tensor(180.9841, dtype=torch.float64)\n",
      "Re tau:tensor(178.0156, dtype=torch.float64)\n",
      "Re tau:tensor(181.0647, dtype=torch.float64)\n",
      "Re tau:tensor(178.3000, dtype=torch.float64)\n",
      "Re tau:tensor(182.7884, dtype=torch.float64)\n",
      "Re tau:tensor(179.6515, dtype=torch.float64)\n",
      "Re tau:tensor(182.8352, dtype=torch.float64)\n",
      "Re tau:tensor(179.9572, dtype=torch.float64)\n",
      "Re tau:tensor(184.2440, dtype=torch.float64)\n",
      "Re tau:tensor(181.2484, dtype=torch.float64)\n",
      "Re tau:tensor(184.1732, dtype=torch.float64)\n",
      "Re tau:tensor(181.5686, dtype=torch.float64)\n",
      "Re tau:tensor(185.8073, dtype=torch.float64)\n",
      "Re tau:tensor(182.9008, dtype=torch.float64)\n",
      "Re tau:tensor(185.7952, dtype=torch.float64)\n",
      "Re tau:tensor(183.3037, dtype=torch.float64)\n",
      "Re tau:tensor(187.3425, dtype=torch.float64)\n",
      "Re tau:tensor(184.4538, dtype=torch.float64)\n",
      "Re tau:tensor(187.4580, dtype=torch.float64)\n",
      "Re tau:tensor(184.8211, dtype=torch.float64)\n",
      "Re tau:tensor(188.8841, dtype=torch.float64)\n",
      "Re tau:tensor(186.0332, dtype=torch.float64)\n",
      "Re tau:tensor(189.2658, dtype=torch.float64)\n",
      "Re tau:tensor(186.6457, dtype=torch.float64)\n",
      "Re tau:tensor(190.6293, dtype=torch.float64)\n",
      "Re tau:tensor(187.7465, dtype=torch.float64)\n",
      "Re tau:tensor(190.6095, dtype=torch.float64)\n",
      "Re tau:tensor(188.2069, dtype=torch.float64)\n",
      "Re tau:tensor(192.3610, dtype=torch.float64)\n",
      "Re tau:tensor(189.4217, dtype=torch.float64)\n",
      "Re tau:tensor(192.6325, dtype=torch.float64)\n",
      "Re tau:tensor(190.0675, dtype=torch.float64)\n",
      "Re tau:tensor(193.8955, dtype=torch.float64)\n",
      "Re tau:tensor(191.1983, dtype=torch.float64)\n",
      "Re tau:tensor(194.2353, dtype=torch.float64)\n",
      "Re tau:tensor(191.7621, dtype=torch.float64)\n",
      "Re tau:tensor(195.7792, dtype=torch.float64)\n",
      "Re tau:tensor(192.7727, dtype=torch.float64)\n",
      "Re tau:tensor(195.8662, dtype=torch.float64)\n",
      "Re tau:tensor(193.2895, dtype=torch.float64)\n",
      "Re tau:tensor(197.3603, dtype=torch.float64)\n",
      "Re tau:tensor(194.4381, dtype=torch.float64)\n",
      "Re tau:tensor(197.8281, dtype=torch.float64)\n",
      "Re tau:tensor(194.9320, dtype=torch.float64)\n",
      "Re tau:tensor(198.8508, dtype=torch.float64)\n",
      "Re tau:tensor(195.8197, dtype=torch.float64)\n",
      "Re tau:tensor(198.9806, dtype=torch.float64)\n",
      "Re tau:tensor(196.3645, dtype=torch.float64)\n",
      "Re tau:tensor(200.6642, dtype=torch.float64)\n",
      "Re tau:tensor(197.4424, dtype=torch.float64)\n",
      "Re tau:tensor(201.0395, dtype=torch.float64)\n",
      "Re tau:tensor(198.0627, dtype=torch.float64)\n",
      "Re tau:tensor(202.0943, dtype=torch.float64)\n",
      "Re tau:tensor(198.9908, dtype=torch.float64)\n",
      "Re tau:tensor(202.3639, dtype=torch.float64)\n",
      "Re tau:tensor(199.6059, dtype=torch.float64)\n",
      "Re tau:tensor(203.8418, dtype=torch.float64)\n",
      "Re tau:tensor(200.5591, dtype=torch.float64)\n",
      "Re tau:tensor(204.0907, dtype=torch.float64)\n",
      "Re tau:tensor(201.1527, dtype=torch.float64)\n",
      "Re tau:tensor(205.1473, dtype=torch.float64)\n",
      "Re tau:tensor(202.1178, dtype=torch.float64)\n",
      "Re tau:tensor(205.6248, dtype=torch.float64)\n",
      "Re tau:tensor(202.7561, dtype=torch.float64)\n",
      "Re tau:tensor(206.9265, dtype=torch.float64)\n",
      "Re tau:tensor(203.6002, dtype=torch.float64)\n",
      "Re tau:tensor(206.9699, dtype=torch.float64)\n",
      "Re tau:tensor(204.1651, dtype=torch.float64)\n",
      "Re tau:tensor(207.9861, dtype=torch.float64)\n",
      "Re tau:tensor(205.0924, dtype=torch.float64)\n",
      "Re tau:tensor(208.4454, dtype=torch.float64)\n",
      "Re tau:tensor(205.6231, dtype=torch.float64)\n",
      "Re tau:tensor(209.5117, dtype=torch.float64)\n",
      "Re tau:tensor(206.4014, dtype=torch.float64)\n",
      "Re tau:tensor(209.5647, dtype=torch.float64)\n",
      "Re tau:tensor(206.9020, dtype=torch.float64)\n",
      "Re tau:tensor(210.5967, dtype=torch.float64)\n",
      "Re tau:tensor(207.7577, dtype=torch.float64)\n",
      "Re tau:tensor(211.0886, dtype=torch.float64)\n",
      "Re tau:tensor(208.3923, dtype=torch.float64)\n",
      "Re tau:tensor(212.0554, dtype=torch.float64)\n",
      "Re tau:tensor(209.1434, dtype=torch.float64)\n",
      "Re tau:tensor(212.1387, dtype=torch.float64)\n",
      "Re tau:tensor(209.6939, dtype=torch.float64)\n",
      "Re tau:tensor(213.2083, dtype=torch.float64)\n",
      "Re tau:tensor(210.3872, dtype=torch.float64)\n",
      "Re tau:tensor(213.7052, dtype=torch.float64)\n",
      "Re tau:tensor(211.0588, dtype=torch.float64)\n",
      "Re tau:tensor(214.7804, dtype=torch.float64)\n",
      "Re tau:tensor(211.8459, dtype=torch.float64)\n",
      "Re tau:tensor(214.9655, dtype=torch.float64)\n",
      "Re tau:tensor(212.4380, dtype=torch.float64)\n",
      "Re tau:tensor(215.9742, dtype=torch.float64)\n",
      "Re tau:tensor(213.1045, dtype=torch.float64)\n",
      "Re tau:tensor(216.5383, dtype=torch.float64)\n",
      "Re tau:tensor(213.7698, dtype=torch.float64)\n",
      "Re tau:tensor(217.5822, dtype=torch.float64)\n",
      "Re tau:tensor(214.5341, dtype=torch.float64)\n",
      "Re tau:tensor(217.8472, dtype=torch.float64)\n",
      "Re tau:tensor(215.1465, dtype=torch.float64)\n",
      "Re tau:tensor(218.9000, dtype=torch.float64)\n",
      "Re tau:tensor(215.8795, dtype=torch.float64)\n",
      "Re tau:tensor(219.4247, dtype=torch.float64)\n",
      "Re tau:tensor(216.5560, dtype=torch.float64)\n",
      "Re tau:tensor(220.5069, dtype=torch.float64)\n",
      "Re tau:tensor(217.3604, dtype=torch.float64)\n",
      "Re tau:tensor(220.9294, dtype=torch.float64)\n",
      "Re tau:tensor(218.1141, dtype=torch.float64)\n",
      "Re tau:tensor(221.9428, dtype=torch.float64)\n",
      "Re tau:tensor(218.8443, dtype=torch.float64)\n",
      "Re tau:tensor(222.5355, dtype=torch.float64)\n",
      "Re tau:tensor(219.5660, dtype=torch.float64)\n",
      "Re tau:tensor(223.5214, dtype=torch.float64)\n",
      "Re tau:tensor(220.2882, dtype=torch.float64)\n",
      "Re tau:tensor(223.8830, dtype=torch.float64)\n",
      "Re tau:tensor(220.9921, dtype=torch.float64)\n",
      "Re tau:tensor(224.8557, dtype=torch.float64)\n",
      "Re tau:tensor(221.8042, dtype=torch.float64)\n",
      "Re tau:tensor(225.3133, dtype=torch.float64)\n",
      "Re tau:tensor(222.3618, dtype=torch.float64)\n",
      "Re tau:tensor(226.2769, dtype=torch.float64)\n",
      "Re tau:tensor(223.0791, dtype=torch.float64)\n",
      "Re tau:tensor(226.6053, dtype=torch.float64)\n",
      "Re tau:tensor(223.7096, dtype=torch.float64)\n",
      "Re tau:tensor(227.4216, dtype=torch.float64)\n",
      "Re tau:tensor(224.3068, dtype=torch.float64)\n",
      "Re tau:tensor(227.8768, dtype=torch.float64)\n",
      "Re tau:tensor(225.0886, dtype=torch.float64)\n",
      "Re tau:tensor(228.7745, dtype=torch.float64)\n",
      "Re tau:tensor(225.7189, dtype=torch.float64)\n",
      "Re tau:tensor(229.1953, dtype=torch.float64)\n",
      "Re tau:tensor(226.2632, dtype=torch.float64)\n",
      "Re tau:tensor(230.0589, dtype=torch.float64)\n",
      "Re tau:tensor(227.0850, dtype=torch.float64)\n",
      "Re tau:tensor(230.4941, dtype=torch.float64)\n",
      "Re tau:tensor(227.7296, dtype=torch.float64)\n",
      "Re tau:tensor(231.4317, dtype=torch.float64)\n",
      "Re tau:tensor(228.2576, dtype=torch.float64)\n",
      "Re tau:tensor(231.8895, dtype=torch.float64)\n",
      "Re tau:tensor(229.0772, dtype=torch.float64)\n",
      "Re tau:tensor(232.7100, dtype=torch.float64)\n",
      "Re tau:tensor(229.7596, dtype=torch.float64)\n",
      "Re tau:tensor(233.1809, dtype=torch.float64)\n",
      "Re tau:tensor(230.3093, dtype=torch.float64)\n",
      "Re tau:tensor(234.1330, dtype=torch.float64)\n",
      "Re tau:tensor(231.0852, dtype=torch.float64)\n",
      "Re tau:tensor(234.5577, dtype=torch.float64)\n",
      "Re tau:tensor(231.7873, dtype=torch.float64)\n",
      "Re tau:tensor(235.3795, dtype=torch.float64)\n",
      "Re tau:tensor(232.2275, dtype=torch.float64)\n",
      "Re tau:tensor(235.9093, dtype=torch.float64)\n",
      "Re tau:tensor(232.9252, dtype=torch.float64)\n",
      "Re tau:tensor(236.7631, dtype=torch.float64)\n",
      "Re tau:tensor(233.7521, dtype=torch.float64)\n",
      "Re tau:tensor(237.1906, dtype=torch.float64)\n",
      "Re tau:tensor(234.2558, dtype=torch.float64)\n",
      "Re tau:tensor(238.0228, dtype=torch.float64)\n",
      "Re tau:tensor(234.7792, dtype=torch.float64)\n",
      "Re tau:tensor(238.4725, dtype=torch.float64)\n",
      "Re tau:tensor(235.6423, dtype=torch.float64)\n",
      "Re tau:tensor(239.3276, dtype=torch.float64)\n",
      "Re tau:tensor(236.3129, dtype=torch.float64)\n",
      "Re tau:tensor(239.7719, dtype=torch.float64)\n",
      "Re tau:tensor(236.7744, dtype=torch.float64)\n",
      "Re tau:tensor(240.6026, dtype=torch.float64)\n",
      "Re tau:tensor(237.4006, dtype=torch.float64)\n",
      "Re tau:tensor(241.0651, dtype=torch.float64)\n",
      "Re tau:tensor(238.2680, dtype=torch.float64)\n",
      "Re tau:tensor(241.8558, dtype=torch.float64)\n",
      "Re tau:tensor(238.8259, dtype=torch.float64)\n",
      "Re tau:tensor(242.3597, dtype=torch.float64)\n",
      "Re tau:tensor(239.4244, dtype=torch.float64)\n",
      "Re tau:tensor(243.2142, dtype=torch.float64)\n",
      "Re tau:tensor(240.1658, dtype=torch.float64)\n",
      "Re tau:tensor(243.6516, dtype=torch.float64)\n",
      "Re tau:tensor(240.9270, dtype=torch.float64)\n",
      "Re tau:tensor(244.4624, dtype=torch.float64)\n",
      "Re tau:tensor(241.4572, dtype=torch.float64)\n",
      "Re tau:tensor(244.9973, dtype=torch.float64)\n",
      "Re tau:tensor(242.1997, dtype=torch.float64)\n",
      "Re tau:tensor(245.8223, dtype=torch.float64)\n",
      "Re tau:tensor(242.9157, dtype=torch.float64)\n",
      "Re tau:tensor(246.2353, dtype=torch.float64)\n",
      "Re tau:tensor(243.4488, dtype=torch.float64)\n",
      "Re tau:tensor(247.0346, dtype=torch.float64)\n",
      "Re tau:tensor(243.9102, dtype=torch.float64)\n",
      "Re tau:tensor(247.5982, dtype=torch.float64)\n",
      "Re tau:tensor(244.7767, dtype=torch.float64)\n",
      "Re tau:tensor(248.4046, dtype=torch.float64)\n",
      "Re tau:tensor(245.4791, dtype=torch.float64)\n",
      "Re tau:tensor(248.8227, dtype=torch.float64)\n",
      "Re tau:tensor(245.9570, dtype=torch.float64)\n",
      "Re tau:tensor(249.6290, dtype=torch.float64)\n",
      "Re tau:tensor(246.4609, dtype=torch.float64)\n",
      "Re tau:tensor(250.2198, dtype=torch.float64)\n",
      "Re tau:tensor(247.3481, dtype=torch.float64)\n",
      "Re tau:tensor(251.0711, dtype=torch.float64)\n",
      "Re tau:tensor(248.0570, dtype=torch.float64)\n",
      "Re tau:tensor(251.6234, dtype=torch.float64)\n",
      "Re tau:tensor(248.5804, dtype=torch.float64)\n",
      "Re tau:tensor(252.3732, dtype=torch.float64)\n",
      "Re tau:tensor(249.0686, dtype=torch.float64)\n",
      "Re tau:tensor(252.9154, dtype=torch.float64)\n",
      "Re tau:tensor(249.8715, dtype=torch.float64)\n",
      "Re tau:tensor(253.7435, dtype=torch.float64)\n",
      "Re tau:tensor(250.5473, dtype=torch.float64)\n",
      "Re tau:tensor(254.1814, dtype=torch.float64)\n",
      "Re tau:tensor(251.0305, dtype=torch.float64)\n",
      "Re tau:tensor(254.9258, dtype=torch.float64)\n",
      "Re tau:tensor(251.5754, dtype=torch.float64)\n",
      "Re tau:tensor(255.3852, dtype=torch.float64)\n",
      "Re tau:tensor(252.2903, dtype=torch.float64)\n",
      "Re tau:tensor(256.1519, dtype=torch.float64)\n",
      "Re tau:tensor(252.8771, dtype=torch.float64)\n",
      "Re tau:tensor(256.6512, dtype=torch.float64)\n",
      "Re tau:tensor(253.4307, dtype=torch.float64)\n",
      "Re tau:tensor(257.2886, dtype=torch.float64)\n",
      "Re tau:tensor(253.9243, dtype=torch.float64)\n",
      "Re tau:tensor(257.7617, dtype=torch.float64)\n",
      "Re tau:tensor(254.6535, dtype=torch.float64)\n",
      "Re tau:tensor(258.4867, dtype=torch.float64)\n",
      "Re tau:tensor(255.1737, dtype=torch.float64)\n",
      "Re tau:tensor(258.8267, dtype=torch.float64)\n",
      "Re tau:tensor(255.7178, dtype=torch.float64)\n",
      "Re tau:tensor(259.6326, dtype=torch.float64)\n",
      "Re tau:tensor(256.3125, dtype=torch.float64)\n",
      "Re tau:tensor(260.0461, dtype=torch.float64)\n",
      "Re tau:tensor(256.9585, dtype=torch.float64)\n",
      "Re tau:tensor(260.6216, dtype=torch.float64)\n",
      "Re tau:tensor(257.4174, dtype=torch.float64)\n",
      "Re tau:tensor(261.0900, dtype=torch.float64)\n",
      "Re tau:tensor(257.9583, dtype=torch.float64)\n",
      "Re tau:tensor(261.7165, dtype=torch.float64)\n",
      "Re tau:tensor(258.4674, dtype=torch.float64)\n",
      "Re tau:tensor(262.0797, dtype=torch.float64)\n",
      "Re tau:tensor(259.1006, dtype=torch.float64)\n",
      "Re tau:tensor(262.8017, dtype=torch.float64)\n",
      "Re tau:tensor(259.6173, dtype=torch.float64)\n",
      "Re tau:tensor(263.0741, dtype=torch.float64)\n",
      "Re tau:tensor(260.0102, dtype=torch.float64)\n",
      "Re tau:tensor(263.6763, dtype=torch.float64)\n",
      "Re tau:tensor(260.5216, dtype=torch.float64)\n",
      "Re tau:tensor(264.2599, dtype=torch.float64)\n",
      "Re tau:tensor(261.2889, dtype=torch.float64)\n",
      "Re tau:tensor(264.9116, dtype=torch.float64)\n",
      "Re tau:tensor(261.8115, dtype=torch.float64)\n",
      "Re tau:tensor(265.1895, dtype=torch.float64)\n",
      "Re tau:tensor(262.2988, dtype=torch.float64)\n",
      "Re tau:tensor(266.0019, dtype=torch.float64)\n",
      "Re tau:tensor(262.8577, dtype=torch.float64)\n",
      "Re tau:tensor(266.5129, dtype=torch.float64)\n",
      "Re tau:tensor(263.5302, dtype=torch.float64)\n",
      "Re tau:tensor(267.1038, dtype=torch.float64)\n",
      "Re tau:tensor(264.1041, dtype=torch.float64)\n",
      "Re tau:tensor(267.5814, dtype=torch.float64)\n",
      "Re tau:tensor(264.6579, dtype=torch.float64)\n",
      "Re tau:tensor(268.2940, dtype=torch.float64)\n",
      "Re tau:tensor(265.1442, dtype=torch.float64)\n",
      "Re tau:tensor(268.6416, dtype=torch.float64)\n",
      "Re tau:tensor(265.7618, dtype=torch.float64)\n",
      "Re tau:tensor(269.3589, dtype=torch.float64)\n",
      "Re tau:tensor(266.3741, dtype=torch.float64)\n",
      "Re tau:tensor(269.9506, dtype=torch.float64)\n",
      "Re tau:tensor(266.9301, dtype=torch.float64)\n",
      "Re tau:tensor(270.5160, dtype=torch.float64)\n",
      "Re tau:tensor(267.3382, dtype=torch.float64)\n",
      "Re tau:tensor(270.8674, dtype=torch.float64)\n",
      "Re tau:tensor(267.9520, dtype=torch.float64)\n",
      "Re tau:tensor(271.7104, dtype=torch.float64)\n",
      "Re tau:tensor(268.5497, dtype=torch.float64)\n",
      "Re tau:tensor(272.2352, dtype=torch.float64)\n",
      "Re tau:tensor(269.0941, dtype=torch.float64)\n",
      "Re tau:tensor(272.7791, dtype=torch.float64)\n",
      "Re tau:tensor(269.5806, dtype=torch.float64)\n",
      "Re tau:tensor(273.2651, dtype=torch.float64)\n",
      "Re tau:tensor(270.1800, dtype=torch.float64)\n",
      "Re tau:tensor(274.0724, dtype=torch.float64)\n",
      "Re tau:tensor(270.7611, dtype=torch.float64)\n",
      "Re tau:tensor(274.4119, dtype=torch.float64)\n",
      "Re tau:tensor(271.2947, dtype=torch.float64)\n",
      "Re tau:tensor(275.0225, dtype=torch.float64)\n",
      "Re tau:tensor(271.7889, dtype=torch.float64)\n",
      "Re tau:tensor(275.5927, dtype=torch.float64)\n",
      "Re tau:tensor(272.3608, dtype=torch.float64)\n",
      "Re tau:tensor(276.2186, dtype=torch.float64)\n",
      "Re tau:tensor(272.8497, dtype=torch.float64)\n",
      "Re tau:tensor(276.4395, dtype=torch.float64)\n",
      "Re tau:tensor(273.3556, dtype=torch.float64)\n",
      "Re tau:tensor(277.1244, dtype=torch.float64)\n",
      "Re tau:tensor(273.8808, dtype=torch.float64)\n",
      "Re tau:tensor(277.6776, dtype=torch.float64)\n",
      "Re tau:tensor(274.3853, dtype=torch.float64)\n",
      "Re tau:tensor(278.2017, dtype=torch.float64)\n",
      "Re tau:tensor(274.9033, dtype=torch.float64)\n",
      "Re tau:tensor(278.4505, dtype=torch.float64)\n",
      "Re tau:tensor(275.4662, dtype=torch.float64)\n",
      "Re tau:tensor(279.2154, dtype=torch.float64)\n",
      "Re tau:tensor(276.0423, dtype=torch.float64)\n",
      "Re tau:tensor(279.7384, dtype=torch.float64)\n",
      "Re tau:tensor(276.5798, dtype=torch.float64)\n",
      "Re tau:tensor(280.2322, dtype=torch.float64)\n",
      "Re tau:tensor(277.0848, dtype=torch.float64)\n",
      "Re tau:tensor(280.6137, dtype=torch.float64)\n",
      "Re tau:tensor(277.7083, dtype=torch.float64)\n",
      "Re tau:tensor(281.3306, dtype=torch.float64)\n",
      "Re tau:tensor(278.1935, dtype=torch.float64)\n",
      "Re tau:tensor(281.7441, dtype=torch.float64)\n",
      "Re tau:tensor(278.6902, dtype=torch.float64)\n",
      "Re tau:tensor(282.3082, dtype=torch.float64)\n",
      "Re tau:tensor(279.2534, dtype=torch.float64)\n",
      "Re tau:tensor(282.7261, dtype=torch.float64)\n",
      "Re tau:tensor(279.7644, dtype=torch.float64)\n",
      "Re tau:tensor(283.4490, dtype=torch.float64)\n",
      "Re tau:tensor(280.3240, dtype=torch.float64)\n",
      "Re tau:tensor(283.7640, dtype=torch.float64)\n",
      "Re tau:tensor(280.7230, dtype=torch.float64)\n",
      "Re tau:tensor(284.3673, dtype=torch.float64)\n",
      "Re tau:tensor(281.2612, dtype=torch.float64)\n",
      "Re tau:tensor(284.9346, dtype=torch.float64)\n",
      "Re tau:tensor(281.8819, dtype=torch.float64)\n",
      "Re tau:tensor(285.5916, dtype=torch.float64)\n",
      "Re tau:tensor(282.3082, dtype=torch.float64)\n",
      "Re tau:tensor(285.9157, dtype=torch.float64)\n",
      "Re tau:tensor(282.8411, dtype=torch.float64)\n",
      "Re tau:tensor(286.5523, dtype=torch.float64)\n",
      "Re tau:tensor(283.3883, dtype=torch.float64)\n",
      "Re tau:tensor(287.0858, dtype=torch.float64)\n",
      "Re tau:tensor(283.8749, dtype=torch.float64)\n",
      "Re tau:tensor(287.7762, dtype=torch.float64)\n",
      "Re tau:tensor(284.4740, dtype=torch.float64)\n",
      "Re tau:tensor(288.0290, dtype=torch.float64)\n",
      "Re tau:tensor(284.8928, dtype=torch.float64)\n",
      "Re tau:tensor(288.6555, dtype=torch.float64)\n",
      "Re tau:tensor(285.3436, dtype=torch.float64)\n",
      "Re tau:tensor(289.2316, dtype=torch.float64)\n",
      "Re tau:tensor(285.9634, dtype=torch.float64)\n",
      "Re tau:tensor(289.8240, dtype=torch.float64)\n",
      "Re tau:tensor(286.4530, dtype=torch.float64)\n",
      "Re tau:tensor(290.0926, dtype=torch.float64)\n",
      "Re tau:tensor(286.8615, dtype=torch.float64)\n",
      "Re tau:tensor(290.7143, dtype=torch.float64)\n",
      "Re tau:tensor(287.4294, dtype=torch.float64)\n",
      "Re tau:tensor(291.2195, dtype=torch.float64)\n",
      "Re tau:tensor(287.9254, dtype=torch.float64)\n",
      "Re tau:tensor(291.8727, dtype=torch.float64)\n",
      "Re tau:tensor(288.4461, dtype=torch.float64)\n",
      "Re tau:tensor(292.1916, dtype=torch.float64)\n",
      "Re tau:tensor(289.0162, dtype=torch.float64)\n",
      "Re tau:tensor(292.7663, dtype=torch.float64)\n",
      "Re tau:tensor(289.4923, dtype=torch.float64)\n",
      "Re tau:tensor(293.2777, dtype=torch.float64)\n",
      "Re tau:tensor(289.9415, dtype=torch.float64)\n",
      "Re tau:tensor(293.9117, dtype=torch.float64)\n",
      "Re tau:tensor(290.6011, dtype=torch.float64)\n",
      "Re tau:tensor(294.2157, dtype=torch.float64)\n",
      "Re tau:tensor(291.0890, dtype=torch.float64)\n",
      "Re tau:tensor(294.7592, dtype=torch.float64)\n",
      "Re tau:tensor(291.5159, dtype=torch.float64)\n",
      "Re tau:tensor(295.2918, dtype=torch.float64)\n",
      "Re tau:tensor(292.1080, dtype=torch.float64)\n",
      "Re tau:tensor(295.8969, dtype=torch.float64)\n",
      "Re tau:tensor(292.6783, dtype=torch.float64)\n",
      "Re tau:tensor(296.2141, dtype=torch.float64)\n",
      "Re tau:tensor(293.0699, dtype=torch.float64)\n",
      "Re tau:tensor(296.7786, dtype=torch.float64)\n",
      "Re tau:tensor(293.6241, dtype=torch.float64)\n",
      "Re tau:tensor(297.2473, dtype=torch.float64)\n",
      "Re tau:tensor(294.1534, dtype=torch.float64)\n",
      "Re tau:tensor(297.7829, dtype=torch.float64)\n",
      "Re tau:tensor(294.5584, dtype=torch.float64)\n",
      "Re tau:tensor(298.1242, dtype=torch.float64)\n",
      "Re tau:tensor(294.9873, dtype=torch.float64)\n",
      "Re tau:tensor(298.6684, dtype=torch.float64)\n",
      "Re tau:tensor(295.5228, dtype=torch.float64)\n",
      "Re tau:tensor(299.0629, dtype=torch.float64)\n",
      "Re tau:tensor(295.9060, dtype=torch.float64)\n",
      "Re tau:tensor(299.6241, dtype=torch.float64)\n",
      "Re tau:tensor(296.2851, dtype=torch.float64)\n",
      "Re tau:tensor(299.9962, dtype=torch.float64)\n",
      "Re tau:tensor(296.8366, dtype=torch.float64)\n",
      "Re tau:tensor(300.5528, dtype=torch.float64)\n",
      "Re tau:tensor(297.3265, dtype=torch.float64)\n",
      "Re tau:tensor(300.9600, dtype=torch.float64)\n",
      "Re tau:tensor(297.6469, dtype=torch.float64)\n",
      "Re tau:tensor(301.5676, dtype=torch.float64)\n",
      "Re tau:tensor(298.1424, dtype=torch.float64)\n",
      "Re tau:tensor(301.9307, dtype=torch.float64)\n",
      "Re tau:tensor(298.7204, dtype=torch.float64)\n",
      "Re tau:tensor(302.4778, dtype=torch.float64)\n",
      "Re tau:tensor(299.1320, dtype=torch.float64)\n",
      "Re tau:tensor(302.8845, dtype=torch.float64)\n",
      "Re tau:tensor(299.5123, dtype=torch.float64)\n",
      "Re tau:tensor(303.4488, dtype=torch.float64)\n",
      "Re tau:tensor(300.0558, dtype=torch.float64)\n",
      "Re tau:tensor(303.7287, dtype=torch.float64)\n",
      "Re tau:tensor(300.4695, dtype=torch.float64)\n",
      "Re tau:tensor(304.2630, dtype=torch.float64)\n",
      "Re tau:tensor(300.8244, dtype=torch.float64)\n",
      "Re tau:tensor(304.6476, dtype=torch.float64)\n",
      "Re tau:tensor(301.2918, dtype=torch.float64)\n",
      "Re tau:tensor(305.1612, dtype=torch.float64)\n",
      "Re tau:tensor(301.8177, dtype=torch.float64)\n",
      "Re tau:tensor(305.3880, dtype=torch.float64)\n",
      "Re tau:tensor(302.1261, dtype=torch.float64)\n",
      "Re tau:tensor(305.9001, dtype=torch.float64)\n",
      "Re tau:tensor(302.5144, dtype=torch.float64)\n",
      "Re tau:tensor(306.4002, dtype=torch.float64)\n",
      "Re tau:tensor(303.1496, dtype=torch.float64)\n",
      "Re tau:tensor(306.8869, dtype=torch.float64)\n",
      "Re tau:tensor(303.6372, dtype=torch.float64)\n",
      "Re tau:tensor(307.2299, dtype=torch.float64)\n",
      "Re tau:tensor(303.9579, dtype=torch.float64)\n",
      "Re tau:tensor(307.7271, dtype=torch.float64)\n",
      "Re tau:tensor(304.4658, dtype=torch.float64)\n",
      "Re tau:tensor(308.1493, dtype=torch.float64)\n",
      "Re tau:tensor(305.0747, dtype=torch.float64)\n",
      "Re tau:tensor(308.7677, dtype=torch.float64)\n",
      "Re tau:tensor(305.5384, dtype=torch.float64)\n",
      "Re tau:tensor(309.0469, dtype=torch.float64)\n",
      "Re tau:tensor(305.8482, dtype=torch.float64)\n",
      "Re tau:tensor(309.5061, dtype=torch.float64)\n",
      "Re tau:tensor(306.3882, dtype=torch.float64)\n",
      "Re tau:tensor(309.9135, dtype=torch.float64)\n",
      "Re tau:tensor(306.8889, dtype=torch.float64)\n",
      "Re tau:tensor(310.3777, dtype=torch.float64)\n",
      "Re tau:tensor(307.1937, dtype=torch.float64)\n",
      "Re tau:tensor(310.8241, dtype=torch.float64)\n",
      "Re tau:tensor(307.6080, dtype=torch.float64)\n",
      "Re tau:tensor(311.2603, dtype=torch.float64)\n",
      "Re tau:tensor(308.1148, dtype=torch.float64)\n",
      "Re tau:tensor(311.5396, dtype=torch.float64)\n",
      "Re tau:tensor(308.5155, dtype=torch.float64)\n",
      "Re tau:tensor(312.1597, dtype=torch.float64)\n",
      "Re tau:tensor(308.8686, dtype=torch.float64)\n",
      "Re tau:tensor(312.6124, dtype=torch.float64)\n",
      "Re tau:tensor(309.3271, dtype=torch.float64)\n",
      "Re tau:tensor(313.0585, dtype=torch.float64)\n",
      "Re tau:tensor(309.8754, dtype=torch.float64)\n",
      "Re tau:tensor(313.4594, dtype=torch.float64)\n",
      "Re tau:tensor(310.2529, dtype=torch.float64)\n",
      "Re tau:tensor(313.9402, dtype=torch.float64)\n",
      "Re tau:tensor(310.5512, dtype=torch.float64)\n",
      "Re tau:tensor(314.3761, dtype=torch.float64)\n",
      "Re tau:tensor(311.0448, dtype=torch.float64)\n",
      "Re tau:tensor(314.9240, dtype=torch.float64)\n",
      "Re tau:tensor(311.5800, dtype=torch.float64)\n",
      "Re tau:tensor(315.1707, dtype=torch.float64)\n",
      "Re tau:tensor(311.9072, dtype=torch.float64)\n",
      "Re tau:tensor(315.5786, dtype=torch.float64)\n",
      "Re tau:tensor(312.2243, dtype=torch.float64)\n",
      "Re tau:tensor(316.0658, dtype=torch.float64)\n",
      "Re tau:tensor(312.7141, dtype=torch.float64)\n",
      "Re tau:tensor(316.5172, dtype=torch.float64)\n",
      "Re tau:tensor(313.1994, dtype=torch.float64)\n",
      "Re tau:tensor(316.8142, dtype=torch.float64)\n",
      "Re tau:tensor(313.5588, dtype=torch.float64)\n",
      "Re tau:tensor(317.3467, dtype=torch.float64)\n",
      "Re tau:tensor(313.9922, dtype=torch.float64)\n",
      "Re tau:tensor(317.7286, dtype=torch.float64)\n",
      "Re tau:tensor(314.4668, dtype=torch.float64)\n",
      "Re tau:tensor(318.1883, dtype=torch.float64)\n",
      "Re tau:tensor(314.9572, dtype=torch.float64)\n",
      "Re tau:tensor(318.6696, dtype=torch.float64)\n",
      "Re tau:tensor(315.3803, dtype=torch.float64)\n",
      "Re tau:tensor(319.1611, dtype=torch.float64)\n",
      "Re tau:tensor(315.8339, dtype=torch.float64)\n",
      "Re tau:tensor(319.4944, dtype=torch.float64)\n",
      "Re tau:tensor(316.3085, dtype=torch.float64)\n",
      "Re tau:tensor(320.0139, dtype=torch.float64)\n",
      "Re tau:tensor(316.7526, dtype=torch.float64)\n",
      "Re tau:tensor(320.4157, dtype=torch.float64)\n",
      "Re tau:tensor(317.0828, dtype=torch.float64)\n",
      "Re tau:tensor(320.7891, dtype=torch.float64)\n",
      "Re tau:tensor(317.5099, dtype=torch.float64)\n",
      "Re tau:tensor(321.2007, dtype=torch.float64)\n",
      "Re tau:tensor(317.9747, dtype=torch.float64)\n",
      "Re tau:tensor(321.6809, dtype=torch.float64)\n",
      "Re tau:tensor(318.3336, dtype=torch.float64)\n",
      "Re tau:tensor(321.9725, dtype=torch.float64)\n",
      "Re tau:tensor(318.6124, dtype=torch.float64)\n",
      "Re tau:tensor(322.4151, dtype=torch.float64)\n",
      "Re tau:tensor(319.0876, dtype=torch.float64)\n",
      "Re tau:tensor(322.9351, dtype=torch.float64)\n",
      "Re tau:tensor(319.5899, dtype=torch.float64)\n",
      "Re tau:tensor(323.3937, dtype=torch.float64)\n",
      "Re tau:tensor(319.9798, dtype=torch.float64)\n",
      "Re tau:tensor(323.7009, dtype=torch.float64)\n",
      "Re tau:tensor(320.2871, dtype=torch.float64)\n",
      "Re tau:tensor(324.2307, dtype=torch.float64)\n",
      "Re tau:tensor(320.7594, dtype=torch.float64)\n",
      "Re tau:tensor(324.7378, dtype=torch.float64)\n",
      "Re tau:tensor(321.3121, dtype=torch.float64)\n",
      "Re tau:tensor(325.1040, dtype=torch.float64)\n",
      "Re tau:tensor(321.7031, dtype=torch.float64)\n",
      "Re tau:tensor(325.4914, dtype=torch.float64)\n",
      "Re tau:tensor(322.0205, dtype=torch.float64)\n",
      "Re tau:tensor(325.9594, dtype=torch.float64)\n",
      "Re tau:tensor(322.4029, dtype=torch.float64)\n",
      "Re tau:tensor(326.2767, dtype=torch.float64)\n",
      "Re tau:tensor(322.8698, dtype=torch.float64)\n",
      "Re tau:tensor(326.6553, dtype=torch.float64)\n",
      "Re tau:tensor(323.3119, dtype=torch.float64)\n",
      "Re tau:tensor(327.0836, dtype=torch.float64)\n",
      "Re tau:tensor(323.5981, dtype=torch.float64)\n",
      "Re tau:tensor(327.4797, dtype=torch.float64)\n",
      "Re tau:tensor(323.9677, dtype=torch.float64)\n",
      "Re tau:tensor(327.7089, dtype=torch.float64)\n",
      "Re tau:tensor(324.3634, dtype=torch.float64)\n",
      "Re tau:tensor(328.1756, dtype=torch.float64)\n",
      "Re tau:tensor(324.8647, dtype=torch.float64)\n",
      "Re tau:tensor(328.6444, dtype=torch.float64)\n",
      "Re tau:tensor(325.2055, dtype=torch.float64)\n",
      "Re tau:tensor(328.9456, dtype=torch.float64)\n",
      "Re tau:tensor(325.5506, dtype=torch.float64)\n",
      "Re tau:tensor(329.2841, dtype=torch.float64)\n",
      "Re tau:tensor(326.0297, dtype=torch.float64)\n",
      "Re tau:tensor(329.7931, dtype=torch.float64)\n",
      "Re tau:tensor(326.4693, dtype=torch.float64)\n",
      "Re tau:tensor(330.1955, dtype=torch.float64)\n",
      "Re tau:tensor(326.8193, dtype=torch.float64)\n",
      "Re tau:tensor(330.5191, dtype=torch.float64)\n",
      "Re tau:tensor(327.2487, dtype=torch.float64)\n",
      "Re tau:tensor(330.8783, dtype=torch.float64)\n",
      "Re tau:tensor(327.6217, dtype=torch.float64)\n",
      "Re tau:tensor(331.3940, dtype=torch.float64)\n",
      "Re tau:tensor(328.0666, dtype=torch.float64)\n",
      "Re tau:tensor(331.7054, dtype=torch.float64)\n",
      "Re tau:tensor(328.4270, dtype=torch.float64)\n",
      "Re tau:tensor(332.0515, dtype=torch.float64)\n",
      "Re tau:tensor(328.8047, dtype=torch.float64)\n",
      "Re tau:tensor(332.5294, dtype=torch.float64)\n",
      "Re tau:tensor(329.2528, dtype=torch.float64)\n",
      "Re tau:tensor(332.9445, dtype=torch.float64)\n",
      "Re tau:tensor(329.6167, dtype=torch.float64)\n",
      "Re tau:tensor(333.2004, dtype=torch.float64)\n",
      "Re tau:tensor(329.9102, dtype=torch.float64)\n",
      "Re tau:tensor(333.6407, dtype=torch.float64)\n",
      "Re tau:tensor(330.3765, dtype=torch.float64)\n",
      "Re tau:tensor(334.0792, dtype=torch.float64)\n",
      "Re tau:tensor(330.7350, dtype=torch.float64)\n",
      "Re tau:tensor(334.4225, dtype=torch.float64)\n",
      "Re tau:tensor(331.0574, dtype=torch.float64)\n",
      "Re tau:tensor(334.6943, dtype=torch.float64)\n",
      "Re tau:tensor(331.4203, dtype=torch.float64)\n",
      "Re tau:tensor(335.1245, dtype=torch.float64)\n",
      "Re tau:tensor(331.7739, dtype=torch.float64)\n",
      "Re tau:tensor(335.5590, dtype=torch.float64)\n",
      "Re tau:tensor(332.1436, dtype=torch.float64)\n",
      "Re tau:tensor(335.8497, dtype=torch.float64)\n",
      "Re tau:tensor(332.5701, dtype=torch.float64)\n",
      "Re tau:tensor(336.1235, dtype=torch.float64)\n",
      "Re tau:tensor(332.8432, dtype=torch.float64)\n",
      "Re tau:tensor(336.5938, dtype=torch.float64)\n",
      "Re tau:tensor(333.1944, dtype=torch.float64)\n",
      "Re tau:tensor(337.0068, dtype=torch.float64)\n",
      "Re tau:tensor(333.6030, dtype=torch.float64)\n",
      "Re tau:tensor(337.2670, dtype=torch.float64)\n",
      "Re tau:tensor(333.9417, dtype=torch.float64)\n",
      "Re tau:tensor(337.5847, dtype=torch.float64)\n",
      "Re tau:tensor(334.2381, dtype=torch.float64)\n",
      "Re tau:tensor(338.0502, dtype=torch.float64)\n",
      "Re tau:tensor(334.6403, dtype=torch.float64)\n",
      "Re tau:tensor(338.3846, dtype=torch.float64)\n",
      "Re tau:tensor(334.9356, dtype=torch.float64)\n",
      "Re tau:tensor(338.6797, dtype=torch.float64)\n",
      "Re tau:tensor(335.2944, dtype=torch.float64)\n",
      "Re tau:tensor(339.0560, dtype=torch.float64)\n",
      "Re tau:tensor(335.6952, dtype=torch.float64)\n",
      "Re tau:tensor(339.4931, dtype=torch.float64)\n",
      "Re tau:tensor(336.0299, dtype=torch.float64)\n",
      "Re tau:tensor(339.8275, dtype=torch.float64)\n",
      "Re tau:tensor(336.3246, dtype=torch.float64)\n",
      "Re tau:tensor(340.1877, dtype=torch.float64)\n",
      "Re tau:tensor(336.7900, dtype=torch.float64)\n",
      "Re tau:tensor(340.5905, dtype=torch.float64)\n",
      "Re tau:tensor(337.1569, dtype=torch.float64)\n",
      "Re tau:tensor(341.0076, dtype=torch.float64)\n",
      "Re tau:tensor(337.4481, dtype=torch.float64)\n",
      "Re tau:tensor(341.3280, dtype=torch.float64)\n",
      "Re tau:tensor(337.8274, dtype=torch.float64)\n",
      "Re tau:tensor(341.6677, dtype=torch.float64)\n",
      "Re tau:tensor(338.2902, dtype=torch.float64)\n",
      "Re tau:tensor(342.0451, dtype=torch.float64)\n",
      "Re tau:tensor(338.5826, dtype=torch.float64)\n",
      "Re tau:tensor(342.4484, dtype=torch.float64)\n",
      "Re tau:tensor(338.9149, dtype=torch.float64)\n",
      "Re tau:tensor(342.6794, dtype=torch.float64)\n",
      "Re tau:tensor(339.2548, dtype=torch.float64)\n",
      "Re tau:tensor(342.9926, dtype=torch.float64)\n",
      "Re tau:tensor(339.6058, dtype=torch.float64)\n",
      "Re tau:tensor(343.3474, dtype=torch.float64)\n",
      "Re tau:tensor(339.9433, dtype=torch.float64)\n",
      "Re tau:tensor(343.7305, dtype=torch.float64)\n",
      "Re tau:tensor(340.3657, dtype=torch.float64)\n",
      "Re tau:tensor(343.9657, dtype=torch.float64)\n",
      "Re tau:tensor(340.6369, dtype=torch.float64)\n",
      "Re tau:tensor(344.3126, dtype=torch.float64)\n",
      "Re tau:tensor(340.9833, dtype=torch.float64)\n",
      "Re tau:tensor(344.7685, dtype=torch.float64)\n",
      "Re tau:tensor(341.4741, dtype=torch.float64)\n",
      "Re tau:tensor(345.1131, dtype=torch.float64)\n",
      "Re tau:tensor(341.8961, dtype=torch.float64)\n",
      "Re tau:tensor(345.4171, dtype=torch.float64)\n",
      "Re tau:tensor(342.1322, dtype=torch.float64)\n",
      "Re tau:tensor(345.7872, dtype=torch.float64)\n",
      "Re tau:tensor(342.4945, dtype=torch.float64)\n",
      "Re tau:tensor(346.2036, dtype=torch.float64)\n",
      "Re tau:tensor(342.9623, dtype=torch.float64)\n",
      "Re tau:tensor(346.5612, dtype=torch.float64)\n",
      "Re tau:tensor(343.3012, dtype=torch.float64)\n",
      "Re tau:tensor(346.7888, dtype=torch.float64)\n",
      "Re tau:tensor(343.4900, dtype=torch.float64)\n",
      "Re tau:tensor(347.1726, dtype=torch.float64)\n",
      "Re tau:tensor(343.8726, dtype=torch.float64)\n",
      "Re tau:tensor(347.5340, dtype=torch.float64)\n",
      "Re tau:tensor(344.2025, dtype=torch.float64)\n",
      "Re tau:tensor(347.8695, dtype=torch.float64)\n",
      "Re tau:tensor(344.4906, dtype=torch.float64)\n",
      "Re tau:tensor(348.1907, dtype=torch.float64)\n",
      "Re tau:tensor(344.8223, dtype=torch.float64)\n",
      "Re tau:tensor(348.5154, dtype=torch.float64)\n",
      "Re tau:tensor(345.1999, dtype=torch.float64)\n",
      "Re tau:tensor(348.8955, dtype=torch.float64)\n",
      "Re tau:tensor(345.4608, dtype=torch.float64)\n",
      "Re tau:tensor(349.3097, dtype=torch.float64)\n",
      "Re tau:tensor(345.8332, dtype=torch.float64)\n",
      "Re tau:tensor(349.5980, dtype=torch.float64)\n",
      "Re tau:tensor(346.2606, dtype=torch.float64)\n",
      "Re tau:tensor(350.0137, dtype=torch.float64)\n",
      "Re tau:tensor(346.6348, dtype=torch.float64)\n",
      "Re tau:tensor(350.4114, dtype=torch.float64)\n",
      "Re tau:tensor(346.8472, dtype=torch.float64)\n",
      "Re tau:tensor(350.7485, dtype=torch.float64)\n",
      "Re tau:tensor(347.2532, dtype=torch.float64)\n",
      "Re tau:tensor(351.0626, dtype=torch.float64)\n",
      "Re tau:tensor(347.6781, dtype=torch.float64)\n",
      "Re tau:tensor(351.4110, dtype=torch.float64)\n",
      "Re tau:tensor(347.9692, dtype=torch.float64)\n",
      "Re tau:tensor(351.7733, dtype=torch.float64)\n",
      "Re tau:tensor(348.2200, dtype=torch.float64)\n",
      "Re tau:tensor(352.1315, dtype=torch.float64)\n",
      "Re tau:tensor(348.6413, dtype=torch.float64)\n",
      "Re tau:tensor(352.3191, dtype=torch.float64)\n",
      "Re tau:tensor(348.9295, dtype=torch.float64)\n",
      "Re tau:tensor(352.6534, dtype=torch.float64)\n",
      "Re tau:tensor(349.2048, dtype=torch.float64)\n",
      "Re tau:tensor(353.1182, dtype=torch.float64)\n",
      "Re tau:tensor(349.5735, dtype=torch.float64)\n",
      "Re tau:tensor(353.4189, dtype=torch.float64)\n",
      "Re tau:tensor(349.9894, dtype=torch.float64)\n",
      "Re tau:tensor(353.6313, dtype=torch.float64)\n",
      "Re tau:tensor(350.2344, dtype=torch.float64)\n",
      "Re tau:tensor(354.0305, dtype=torch.float64)\n",
      "Re tau:tensor(350.5442, dtype=torch.float64)\n",
      "Re tau:tensor(354.4327, dtype=torch.float64)\n",
      "Re tau:tensor(350.9595, dtype=torch.float64)\n",
      "Re tau:tensor(354.7832, dtype=torch.float64)\n",
      "Re tau:tensor(351.3702, dtype=torch.float64)\n",
      "Re tau:tensor(355.0544, dtype=torch.float64)\n",
      "Re tau:tensor(351.5860, dtype=torch.float64)\n",
      "Re tau:tensor(355.3268, dtype=torch.float64)\n",
      "Re tau:tensor(351.8785, dtype=torch.float64)\n",
      "Re tau:tensor(355.6874, dtype=torch.float64)\n",
      "Re tau:tensor(352.2779, dtype=torch.float64)\n",
      "Re tau:tensor(356.1163, dtype=torch.float64)\n",
      "Re tau:tensor(352.6428, dtype=torch.float64)\n",
      "Re tau:tensor(356.3485, dtype=torch.float64)\n",
      "Re tau:tensor(352.8348, dtype=torch.float64)\n",
      "Re tau:tensor(356.6222, dtype=torch.float64)\n",
      "Re tau:tensor(353.1810, dtype=torch.float64)\n",
      "Re tau:tensor(357.0121, dtype=torch.float64)\n",
      "Re tau:tensor(353.5649, dtype=torch.float64)\n",
      "Re tau:tensor(357.3937, dtype=torch.float64)\n",
      "Re tau:tensor(353.8902, dtype=torch.float64)\n",
      "Re tau:tensor(357.6945, dtype=torch.float64)\n",
      "Re tau:tensor(354.1626, dtype=torch.float64)\n",
      "Re tau:tensor(358.0719, dtype=torch.float64)\n",
      "Re tau:tensor(354.5779, dtype=torch.float64)\n",
      "Re tau:tensor(358.3710, dtype=torch.float64)\n",
      "Re tau:tensor(354.8958, dtype=torch.float64)\n",
      "Re tau:tensor(358.6839, dtype=torch.float64)\n",
      "Re tau:tensor(355.1704, dtype=torch.float64)\n",
      "Re tau:tensor(359.0754, dtype=torch.float64)\n",
      "Re tau:tensor(355.4974, dtype=torch.float64)\n",
      "Re tau:tensor(359.4171, dtype=torch.float64)\n",
      "Re tau:tensor(355.8863, dtype=torch.float64)\n",
      "Re tau:tensor(359.6252, dtype=torch.float64)\n",
      "Re tau:tensor(356.1455, dtype=torch.float64)\n",
      "Re tau:tensor(359.9502, dtype=torch.float64)\n",
      "Re tau:tensor(356.3682, dtype=torch.float64)\n",
      "Re tau:tensor(360.2882, dtype=torch.float64)\n",
      "Re tau:tensor(356.6639, dtype=torch.float64)\n",
      "Re tau:tensor(360.5720, dtype=torch.float64)\n",
      "Re tau:tensor(357.0605, dtype=torch.float64)\n",
      "Re tau:tensor(360.8441, dtype=torch.float64)\n",
      "Re tau:tensor(357.3097, dtype=torch.float64)\n",
      "Re tau:tensor(361.2113, dtype=torch.float64)\n",
      "Re tau:tensor(357.5594, dtype=torch.float64)\n",
      "Re tau:tensor(361.4408, dtype=torch.float64)\n",
      "Re tau:tensor(357.8330, dtype=torch.float64)\n",
      "Re tau:tensor(361.7500, dtype=torch.float64)\n",
      "Re tau:tensor(358.2509, dtype=torch.float64)\n",
      "Re tau:tensor(362.1122, dtype=torch.float64)\n",
      "Re tau:tensor(358.5395, dtype=torch.float64)\n",
      "Re tau:tensor(362.4017, dtype=torch.float64)\n",
      "Re tau:tensor(358.7673, dtype=torch.float64)\n",
      "Re tau:tensor(362.6040, dtype=torch.float64)\n",
      "Re tau:tensor(359.0768, dtype=torch.float64)\n",
      "Re tau:tensor(362.8913, dtype=torch.float64)\n",
      "Re tau:tensor(359.4130, dtype=torch.float64)\n",
      "Re tau:tensor(363.2099, dtype=torch.float64)\n",
      "Re tau:tensor(359.6618, dtype=torch.float64)\n",
      "Re tau:tensor(363.4611, dtype=torch.float64)\n",
      "Re tau:tensor(359.9183, dtype=torch.float64)\n",
      "Re tau:tensor(363.6730, dtype=torch.float64)\n",
      "Re tau:tensor(360.1765, dtype=torch.float64)\n",
      "Re tau:tensor(363.9997, dtype=torch.float64)\n",
      "Re tau:tensor(360.5149, dtype=torch.float64)\n",
      "Re tau:tensor(364.2146, dtype=torch.float64)\n",
      "Re tau:tensor(360.7171, dtype=torch.float64)\n",
      "Re tau:tensor(364.4930, dtype=torch.float64)\n",
      "Re tau:tensor(361.0177, dtype=torch.float64)\n",
      "Re tau:tensor(364.8319, dtype=torch.float64)\n",
      "Re tau:tensor(361.3786, dtype=torch.float64)\n",
      "Re tau:tensor(365.1000, dtype=torch.float64)\n",
      "Re tau:tensor(361.6347, dtype=torch.float64)\n",
      "Re tau:tensor(365.2799, dtype=torch.float64)\n",
      "Re tau:tensor(361.8668, dtype=torch.float64)\n",
      "Re tau:tensor(365.6651, dtype=torch.float64)\n",
      "Re tau:tensor(362.2179, dtype=torch.float64)\n",
      "Re tau:tensor(366.0314, dtype=torch.float64)\n",
      "Re tau:tensor(362.5321, dtype=torch.float64)\n",
      "Re tau:tensor(366.2696, dtype=torch.float64)\n",
      "Re tau:tensor(362.8644, dtype=torch.float64)\n",
      "Re tau:tensor(366.4615, dtype=torch.float64)\n",
      "Re tau:tensor(363.0977, dtype=torch.float64)\n",
      "Re tau:tensor(366.8813, dtype=torch.float64)\n",
      "Re tau:tensor(363.3939, dtype=torch.float64)\n",
      "Re tau:tensor(367.2456, dtype=torch.float64)\n",
      "Re tau:tensor(363.7651, dtype=torch.float64)\n",
      "Re tau:tensor(367.4734, dtype=torch.float64)\n",
      "Re tau:tensor(364.0641, dtype=torch.float64)\n",
      "Re tau:tensor(367.7386, dtype=torch.float64)\n",
      "Re tau:tensor(364.2915, dtype=torch.float64)\n",
      "Re tau:tensor(368.1452, dtype=torch.float64)\n",
      "Re tau:tensor(364.6334, dtype=torch.float64)\n",
      "Re tau:tensor(368.4130, dtype=torch.float64)\n",
      "Re tau:tensor(364.9362, dtype=torch.float64)\n",
      "Re tau:tensor(368.7019, dtype=torch.float64)\n",
      "Re tau:tensor(365.2479, dtype=torch.float64)\n",
      "Re tau:tensor(369.0183, dtype=torch.float64)\n",
      "Re tau:tensor(365.5161, dtype=torch.float64)\n",
      "Re tau:tensor(369.2997, dtype=torch.float64)\n",
      "Re tau:tensor(365.7599, dtype=torch.float64)\n",
      "Re tau:tensor(369.5316, dtype=torch.float64)\n",
      "Re tau:tensor(366.0664, dtype=torch.float64)\n",
      "Re tau:tensor(369.8885, dtype=torch.float64)\n",
      "Re tau:tensor(366.3896, dtype=torch.float64)\n",
      "Re tau:tensor(370.1511, dtype=torch.float64)\n",
      "Re tau:tensor(366.5626, dtype=torch.float64)\n",
      "Re tau:tensor(370.3748, dtype=torch.float64)\n",
      "Re tau:tensor(366.8247, dtype=torch.float64)\n",
      "Re tau:tensor(370.6203, dtype=torch.float64)\n",
      "Re tau:tensor(367.1944, dtype=torch.float64)\n",
      "Re tau:tensor(371.0001, dtype=torch.float64)\n",
      "Re tau:tensor(367.4467, dtype=torch.float64)\n",
      "Re tau:tensor(371.2695, dtype=torch.float64)\n",
      "Re tau:tensor(367.6543, dtype=torch.float64)\n",
      "Re tau:tensor(371.4748, dtype=torch.float64)\n",
      "Re tau:tensor(367.9891, dtype=torch.float64)\n",
      "Re tau:tensor(371.7357, dtype=torch.float64)\n",
      "Re tau:tensor(368.2903, dtype=torch.float64)\n",
      "Re tau:tensor(372.1365, dtype=torch.float64)\n",
      "Re tau:tensor(368.5565, dtype=torch.float64)\n",
      "Re tau:tensor(372.3595, dtype=torch.float64)\n",
      "Re tau:tensor(368.8153, dtype=torch.float64)\n",
      "Re tau:tensor(372.5516, dtype=torch.float64)\n",
      "Re tau:tensor(369.0915, dtype=torch.float64)\n",
      "Re tau:tensor(372.8415, dtype=torch.float64)\n",
      "Re tau:tensor(369.4169, dtype=torch.float64)\n",
      "Re tau:tensor(373.2427, dtype=torch.float64)\n",
      "Re tau:tensor(369.7351, dtype=torch.float64)\n",
      "Re tau:tensor(373.4271, dtype=torch.float64)\n",
      "Re tau:tensor(369.9419, dtype=torch.float64)\n",
      "Re tau:tensor(373.6935, dtype=torch.float64)\n",
      "Re tau:tensor(370.2239, dtype=torch.float64)\n",
      "Re tau:tensor(374.0223, dtype=torch.float64)\n",
      "Re tau:tensor(370.5942, dtype=torch.float64)\n",
      "Re tau:tensor(374.3373, dtype=torch.float64)\n",
      "Re tau:tensor(370.8696, dtype=torch.float64)\n",
      "Re tau:tensor(374.5460, dtype=torch.float64)\n",
      "Re tau:tensor(371.0661, dtype=torch.float64)\n",
      "Re tau:tensor(374.8399, dtype=torch.float64)\n",
      "Re tau:tensor(371.3776, dtype=torch.float64)\n",
      "Re tau:tensor(375.1603, dtype=torch.float64)\n",
      "Re tau:tensor(371.7191, dtype=torch.float64)\n",
      "Re tau:tensor(375.3989, dtype=torch.float64)\n",
      "Re tau:tensor(371.9149, dtype=torch.float64)\n",
      "Re tau:tensor(375.6225, dtype=torch.float64)\n",
      "Re tau:tensor(372.1436, dtype=torch.float64)\n",
      "Re tau:tensor(375.9596, dtype=torch.float64)\n",
      "Re tau:tensor(372.4541, dtype=torch.float64)\n",
      "Re tau:tensor(376.2043, dtype=torch.float64)\n",
      "Re tau:tensor(372.6960, dtype=torch.float64)\n",
      "Re tau:tensor(376.4746, dtype=torch.float64)\n",
      "Re tau:tensor(372.9559, dtype=torch.float64)\n",
      "Re tau:tensor(376.7021, dtype=torch.float64)\n",
      "Re tau:tensor(373.2147, dtype=torch.float64)\n",
      "Re tau:tensor(377.0750, dtype=torch.float64)\n",
      "Re tau:tensor(373.4629, dtype=torch.float64)\n",
      "Re tau:tensor(377.3826, dtype=torch.float64)\n",
      "Re tau:tensor(373.7561, dtype=torch.float64)\n",
      "Re tau:tensor(377.6234, dtype=torch.float64)\n",
      "Re tau:tensor(374.0950, dtype=torch.float64)\n",
      "Re tau:tensor(377.8991, dtype=torch.float64)\n",
      "Re tau:tensor(374.3671, dtype=torch.float64)\n",
      "Re tau:tensor(378.2733, dtype=torch.float64)\n",
      "Re tau:tensor(374.5478, dtype=torch.float64)\n",
      "Re tau:tensor(378.5373, dtype=torch.float64)\n",
      "Re tau:tensor(374.8636, dtype=torch.float64)\n",
      "Re tau:tensor(378.7972, dtype=torch.float64)\n",
      "Re tau:tensor(375.2190, dtype=torch.float64)\n",
      "Re tau:tensor(378.9921, dtype=torch.float64)\n",
      "Re tau:tensor(375.3994, dtype=torch.float64)\n",
      "Re tau:tensor(379.3259, dtype=torch.float64)\n",
      "Re tau:tensor(375.5856, dtype=torch.float64)\n",
      "Re tau:tensor(379.5650, dtype=torch.float64)\n",
      "Re tau:tensor(375.8890, dtype=torch.float64)\n",
      "Re tau:tensor(379.7431, dtype=torch.float64)\n",
      "Re tau:tensor(376.1312, dtype=torch.float64)\n",
      "Re tau:tensor(380.0043, dtype=torch.float64)\n",
      "Re tau:tensor(376.3619, dtype=torch.float64)\n",
      "Re tau:tensor(380.3138, dtype=torch.float64)\n",
      "Re tau:tensor(376.6187, dtype=torch.float64)\n",
      "Re tau:tensor(380.4818, dtype=torch.float64)\n",
      "Re tau:tensor(376.8752, dtype=torch.float64)\n",
      "Re tau:tensor(380.7734, dtype=torch.float64)\n",
      "Re tau:tensor(377.1390, dtype=torch.float64)\n",
      "Re tau:tensor(381.0784, dtype=torch.float64)\n",
      "Re tau:tensor(377.4713, dtype=torch.float64)\n",
      "Re tau:tensor(381.3761, dtype=torch.float64)\n",
      "Re tau:tensor(377.7950, dtype=torch.float64)\n",
      "Re tau:tensor(381.6113, dtype=torch.float64)\n",
      "Re tau:tensor(378.0163, dtype=torch.float64)\n",
      "Re tau:tensor(381.8672, dtype=torch.float64)\n",
      "Re tau:tensor(378.2517, dtype=torch.float64)\n",
      "Re tau:tensor(382.1665, dtype=torch.float64)\n",
      "Re tau:tensor(378.6076, dtype=torch.float64)\n",
      "Re tau:tensor(382.4862, dtype=torch.float64)\n",
      "Re tau:tensor(378.8685, dtype=torch.float64)\n",
      "Re tau:tensor(382.6190, dtype=torch.float64)\n",
      "Re tau:tensor(378.9967, dtype=torch.float64)\n",
      "Re tau:tensor(382.8282, dtype=torch.float64)\n",
      "Re tau:tensor(379.2350, dtype=torch.float64)\n",
      "Re tau:tensor(383.1086, dtype=torch.float64)\n",
      "Re tau:tensor(379.5228, dtype=torch.float64)\n",
      "Re tau:tensor(383.3515, dtype=torch.float64)\n",
      "Re tau:tensor(379.6766, dtype=torch.float64)\n",
      "Re tau:tensor(383.5267, dtype=torch.float64)\n",
      "Re tau:tensor(379.8573, dtype=torch.float64)\n",
      "Re tau:tensor(383.7860, dtype=torch.float64)\n",
      "Re tau:tensor(380.1405, dtype=torch.float64)\n",
      "Re tau:tensor(384.0038, dtype=torch.float64)\n",
      "Re tau:tensor(380.3591, dtype=torch.float64)\n",
      "Re tau:tensor(384.3094, dtype=torch.float64)\n",
      "Re tau:tensor(380.5474, dtype=torch.float64)\n",
      "Re tau:tensor(384.5748, dtype=torch.float64)\n",
      "Re tau:tensor(380.8369, dtype=torch.float64)\n",
      "Re tau:tensor(384.7904, dtype=torch.float64)\n",
      "Re tau:tensor(381.1422, dtype=torch.float64)\n",
      "Re tau:tensor(385.0195, dtype=torch.float64)\n",
      "Re tau:tensor(381.3483, dtype=torch.float64)\n",
      "Re tau:tensor(385.3297, dtype=torch.float64)\n",
      "Re tau:tensor(381.5458, dtype=torch.float64)\n",
      "Re tau:tensor(385.5077, dtype=torch.float64)\n",
      "Re tau:tensor(381.8480, dtype=torch.float64)\n",
      "Re tau:tensor(385.7366, dtype=torch.float64)\n",
      "Re tau:tensor(382.1169, dtype=torch.float64)\n",
      "Re tau:tensor(385.9815, dtype=torch.float64)\n",
      "Re tau:tensor(382.3068, dtype=torch.float64)\n",
      "Re tau:tensor(386.1656, dtype=torch.float64)\n",
      "Re tau:tensor(382.4978, dtype=torch.float64)\n",
      "Re tau:tensor(386.3249, dtype=torch.float64)\n",
      "Re tau:tensor(382.7932, dtype=torch.float64)\n",
      "Re tau:tensor(386.6256, dtype=torch.float64)\n",
      "Re tau:tensor(383.0680, dtype=torch.float64)\n",
      "Re tau:tensor(386.8627, dtype=torch.float64)\n",
      "Re tau:tensor(383.2877, dtype=torch.float64)\n",
      "Re tau:tensor(387.0842, dtype=torch.float64)\n",
      "Re tau:tensor(383.5884, dtype=torch.float64)\n",
      "Re tau:tensor(387.2863, dtype=torch.float64)\n",
      "Re tau:tensor(383.8415, dtype=torch.float64)\n",
      "Re tau:tensor(387.5908, dtype=torch.float64)\n",
      "Re tau:tensor(384.0834, dtype=torch.float64)\n",
      "Re tau:tensor(387.8632, dtype=torch.float64)\n",
      "Re tau:tensor(384.3714, dtype=torch.float64)\n",
      "Re tau:tensor(388.1294, dtype=torch.float64)\n",
      "Re tau:tensor(384.6782, dtype=torch.float64)\n",
      "Re tau:tensor(388.2878, dtype=torch.float64)\n",
      "Re tau:tensor(384.8454, dtype=torch.float64)\n",
      "Re tau:tensor(388.4970, dtype=torch.float64)\n",
      "Re tau:tensor(384.9863, dtype=torch.float64)\n",
      "Re tau:tensor(388.8163, dtype=torch.float64)\n",
      "Re tau:tensor(385.3104, dtype=torch.float64)\n",
      "Re tau:tensor(389.0813, dtype=torch.float64)\n",
      "Re tau:tensor(385.5698, dtype=torch.float64)\n",
      "Re tau:tensor(389.1845, dtype=torch.float64)\n",
      "Re tau:tensor(385.6458, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "# CUDA & Präzision\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "    print(\"CUDA ist verfügbar. Verwende GPU.\")\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    print(\"CUDA ist nicht verfügbar. Verwende CPU.\")\n",
    "\n",
    "dtype = torch.float64  # Für Stabilität bei hohen Re\n",
    "lattice = Lattice(D3Q19(), device=device, dtype=dtype)\n",
    "\n",
    "# 🧱 Domänenmaße & Setup\n",
    "h = args[\"h\"]\n",
    "res_y = 2 * h\n",
    "res_x = int(2 * np.pi * h)\n",
    "res_z = int(np.pi * h)\n",
    "\n",
    "# 👇 Maske für Wandfunktion vorbereiten\n",
    "grid_x, grid_y, grid_z = np.meshgrid(\n",
    "    np.arange(res_x), np.arange(res_y), np.arange(res_z), indexing='ij'\n",
    ")\n",
    "\n",
    "mask_bottom = np.zeros_like(grid_x, dtype=bool)\n",
    "mask_bottom[:, 0, :] = True  # Erste Fluidzelle oben\n",
    "\n",
    "mask_top = np.zeros_like(grid_x, dtype=bool)\n",
    "mask_top[:, -1, :] = True  # Erste Fluidzelle unten\n",
    "\n",
    "# 🔧 Boundaries manuell erzeugen\n",
    "wfb_bottom = WallFunctionBoundaryTest(mask=mask_bottom, lattice=lattice, flow=None, wall='bottom')\n",
    "wfb_top    = WallFunctionBoundaryTest(mask=mask_top,    lattice=lattice, flow=None, wall='top')\n",
    "\n",
    "# 🌊 Flow erzeugen, Boundaries übergeben\n",
    "flow = ChannelFlow3DTest(\n",
    "    resolution_x=res_x,\n",
    "    resolution_y=res_y,\n",
    "    resolution_z=res_z,\n",
    "    reynolds_number=Re,\n",
    "    mach_number=Mach,\n",
    "    lattice=lattice,\n",
    "    char_length_lu=res_y,\n",
    "    boundaries=[wfb_bottom, wfb_top]\n",
    ")\n",
    "\n",
    "# 🧩 Boundaries kennen jetzt den Flow\n",
    "wfb_bottom.flow = flow\n",
    "wfb_top.flow = flow\n",
    "\n",
    "# 🧠 Check: IDs vergleichen\n",
    "\n",
    "# 📈 Reporter: Global Mean Ux\n",
    "global_mean_ux_reporter = lt.GlobalMeanUXReporter(lattice, flow)\n",
    "\n",
    "# 📊 Wall Quantities Reporter (lesen von denselben Objekten)\n",
    "wq_bottom = lt.WallQuantities(lattice=lattice, flow=flow, boundary=wfb_bottom)\n",
    "wq_top = lt.WallQuantities(lattice=lattice, flow=flow, boundary=wfb_top)\n",
    "\n",
    "\n",
    "\n",
    "# 🌀 Adaptive Force mit denselben Boundaries\n",
    "adaptive_force_instance = lt.AdaptiveForce(\n",
    "    lattice=lattice,\n",
    "    flow=flow,\n",
    "    target_u_m_lu=flow.units.convert_velocity_to_lu(1.0),\n",
    "    wall_bottom=wfb_bottom,\n",
    "    wall_top=wfb_top,\n",
    "    global_ux_reporter=global_mean_ux_reporter,\n",
    "    base_lbm_tau_lu=flow.units.relaxation_parameter_lu\n",
    ")\n",
    "\n",
    "# ⚙️ Kollision & Simulation\n",
    "collision = lt.BGKCollision(lattice, tau=flow.units.relaxation_parameter_lu, force=adaptive_force_instance)\n",
    "\n",
    "\n",
    "streaming = lt.StandardStreaming(lattice)\n",
    "\n",
    "\n",
    "\n",
    "simulation = lt.Simulation(flow=flow, lattice=lattice, collision=collision, streaming=streaming)\n",
    "\n",
    "\n",
    "\n",
    "# 📤 Reporter einfügen\n",
    "simulation.reporters.append(lt.ObservableReporter(global_mean_ux_reporter, interval=1, out=None))\n",
    "simulation.reporters.append(lt.ObservableReporter(wq_bottom, interval=1, out=None))\n",
    "simulation.reporters.append(lt.ObservableReporter(wq_top, interval=1, out=None))\n",
    "simulation.reporters.append(lt.ObservableReporter(lt.observables.IncompressibleKineticEnergy(lattice, flow), interval=100, out=None))\n",
    "\n",
    "# 📦 VTK\n",
    "steps = int(flow.units.convert_time_to_lu(tmax))\n",
    "vtk_reporter = lt.VTKReporter(lattice=lattice, flow=flow, interval=max(1, int(steps/100)), filename_base=basedir + \"/output\")\n",
    "simulation.reporters.append(vtk_reporter)\n",
    "\n",
    "# ▶️ Simulation starten\n",
    "simulation.initialize_f_neq()\n",
    "mlups = simulation.step(num_steps=steps)\n",
    "\n",
    "# 🧾 Ergebnisse abspeichern\n",
    "wq_top_arr = np.array(simulation.reporters[2].out)\n",
    "wq_bottom_arr = np.array(simulation.reporters[1].out)\n",
    "ux_mean_arr = np.array(simulation.reporters[0].out)\n",
    "\n",
    "with open(csvdir + 'uxmean.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerows(ux_mean_arr)\n",
    "with open(csvdir + 'WallQuantitiesTop.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerows(wq_top_arr)\n",
    "with open(csvdir + 'WallQuantitiesBottom.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerows(wq_bottom_arr)\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2025-06-25T06:03:27.528555239Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "print(wq_bottom)\n",
    "# Beispiel: Daten laden\n",
    "data = (wq_bottom_arr+wq_top_arr)/2\n",
    "time = data[:, 1]\n",
    "re_tau = data[:, 3]\n",
    "y_plus = data[:, 4]\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(time, re_tau, label=\"Re_tau (bottom)\")\n",
    "plt.xlabel(\"Zeit\")\n",
    "plt.ylabel(\"Re_tau\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.title(\"Re_tau über die Zeit\")\n",
    "plt.savefig(csvdir + \"retau.pdf\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(time, y_plus, label=\"y⁺ (bottom)\")\n",
    "plt.xlabel(\"Zeit\")\n",
    "plt.ylabel(\"y⁺\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.title(\"y⁺ über die Zeit\")\n",
    "plt.savefig(csvdir + \"yplus.pdf\")\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
